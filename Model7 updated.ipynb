{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Simacoder/data-phandas-outbreak-challenge/blob/main/Model7%20updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c70b70c0",
      "metadata": {
        "id": "c70b70c0"
      },
      "source": [
        "# Model7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b95ce335",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b95ce335",
        "outputId": "39e18c13-c5b2-4017-f628-43c8c8087ef4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in training dataset: Index(['ID', 'Total', 'Location', 'Category_Health_Facility_UUID', 'Disease',\n",
            "       'Month', 'Year', 'Transformed_Latitude', 'Transformed_Longitude'],\n",
            "      dtype='object')\n",
            "Columns in testing dataset: Index(['Location', 'Disease', 'Month', 'Category_Health_Facility_UUID', 'Year',\n",
            "       'Transformed_Latitude', 'Transformed_Longitude', 'ID'],\n",
            "      dtype='object')\n",
            "\n",
            "Initial target variable statistics:\n",
            "count    23847.000000\n",
            "mean         8.355600\n",
            "std         28.076713\n",
            "min          0.000000\n",
            "25%          0.000000\n",
            "50%          0.000000\n",
            "75%          3.000000\n",
            "max        489.000000\n",
            "Name: Total, dtype: float64\n",
            "\n",
            "Target variable statistics after cleaning:\n",
            "count    23847.000000\n",
            "mean         8.355600\n",
            "std         28.076713\n",
            "min          0.000000\n",
            "25%          0.000000\n",
            "50%          0.000000\n",
            "75%          3.000000\n",
            "max        489.000000\n",
            "Name: Total, dtype: float64\n",
            "\n",
            "Removed 0 rows with invalid Total values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-796a3c6b0995>:81: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(df[col].median(), inplace=True)\n",
            "<ipython-input-2-796a3c6b0995>:87: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(df[col].mode()[0], inplace=True)\n",
            "<ipython-input-2-796a3c6b0995>:81: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(df[col].median(), inplace=True)\n",
            "<ipython-input-2-796a3c6b0995>:87: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(df[col].mode()[0], inplace=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation MAE: 9.90429589219813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation MAE: 11.14 ± 2.23\n",
            "Predictions saved to 'predictions.csv'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Custom wrapper for XGBRegressor to ensure compatibility\n",
        "class CustomXGBRegressor(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, **params):\n",
        "        self.model = XGBRegressor(**params)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.model.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return self.model.get_params(deep)\n",
        "\n",
        "    def set_params(self, **params):\n",
        "        self.model.set_params(**params)\n",
        "        return self\n",
        "\n",
        "class DiseasePredictionPipeline:\n",
        "    def __init__(self):\n",
        "        self.train = None\n",
        "        self.test = None\n",
        "        self.pipeline = None\n",
        "        self.feature_columns = None\n",
        "\n",
        "    def load_data(self):\n",
        "        # Load datasets\n",
        "        self.train = pd.read_csv(\"Train.csv\")\n",
        "        self.test = pd.read_csv(\"Test.csv\")\n",
        "\n",
        "        print(\"Columns in training dataset:\", self.train.columns)\n",
        "        print(\"Columns in testing dataset:\", self.test.columns)\n",
        "\n",
        "        # Clean target variable\n",
        "        print(\"\\nInitial target variable statistics:\")\n",
        "        print(self.train['Total'].describe())\n",
        "\n",
        "        # Remove rows where Total is NaN, infinite, or extremely large\n",
        "        self.train = self.train[\n",
        "            np.isfinite(self.train['Total'])\n",
        "        ]\n",
        "\n",
        "        print(\"\\nTarget variable statistics after cleaning:\")\n",
        "        print(self.train['Total'].describe())\n",
        "        print(f\"\\nRemoved {len(self.train) - len(self.train)} rows with invalid Total values\")\n",
        "\n",
        "        # Define feature columns (excluding 'Total' and 'ID')\n",
        "        self.feature_columns = [col for col in self.train.columns\n",
        "                              if col not in ['Total', 'ID']]\n",
        "\n",
        "        # Verify all feature columns exist in test dataset\n",
        "        missing_cols = [col for col in self.feature_columns\n",
        "                       if col not in self.test.columns]\n",
        "        if missing_cols:\n",
        "            raise ValueError(f\"Missing columns in test dataset: {missing_cols}\")\n",
        "\n",
        "        # Handle missing values separately for train and test\n",
        "        self._handle_missing_values(self.train)\n",
        "        self._handle_missing_values(self.test)\n",
        "\n",
        "    def _handle_missing_values(self, df):\n",
        "        \"\"\"Handle missing values for a given dataframe\"\"\"\n",
        "        # Numerical columns\n",
        "        numerical_cols = df[self.feature_columns].select_dtypes(\n",
        "            include=['float64', 'int64']).columns\n",
        "        for col in numerical_cols:\n",
        "            # Replace infinite values with NaN first\n",
        "            df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
        "            # Then fill NaN with median\n",
        "            df[col].fillna(df[col].median(), inplace=True)\n",
        "\n",
        "        # Categorical columns\n",
        "        categorical_cols = df[self.feature_columns].select_dtypes(\n",
        "            include=['object']).columns\n",
        "        for col in categorical_cols:\n",
        "            df[col].fillna(df[col].mode()[0], inplace=True)\n",
        "\n",
        "    def create_pipeline(self):\n",
        "        # Define preprocessing for numerical features\n",
        "        numerical_features = [col for col in self.feature_columns\n",
        "                            if self.train[col].dtype in ['float64', 'int64']]\n",
        "        numerical_transformer = Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='median')),\n",
        "            ('scaler', StandardScaler())\n",
        "        ])\n",
        "\n",
        "        # Define preprocessing for categorical features\n",
        "        categorical_features = [col for col in self.feature_columns\n",
        "                              if self.train[col].dtype == 'object']\n",
        "        categorical_transformer = Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "        ])\n",
        "\n",
        "        # Combine preprocessing steps\n",
        "        preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', numerical_transformer, numerical_features),\n",
        "                ('cat', categorical_transformer, categorical_features)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Define the pipeline\n",
        "        self.pipeline = Pipeline(steps=[\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('model', CustomXGBRegressor(\n",
        "                n_estimators=100,\n",
        "                learning_rate=0.1,\n",
        "                max_depth=5,\n",
        "                random_state=42,\n",
        "                objective='count:poisson',  # Better for count data\n",
        "                tree_method='hist',         # Faster training\n",
        "                min_child_weight=1,         # Help with many zero values\n",
        "                subsample=0.8,              # Prevent overfitting\n",
        "                colsample_bytree=0.8        # Prevent overfitting\n",
        "            ))\n",
        "        ])\n",
        "\n",
        "    def run_pipeline(self):\n",
        "        try:\n",
        "            # Load and preprocess data\n",
        "            self.load_data()\n",
        "\n",
        "            # Prepare training data\n",
        "            X = self.train[self.feature_columns]\n",
        "            y = self.train['Total']\n",
        "\n",
        "            # Split the training data\n",
        "            X_train, X_val, y_train, y_val = train_test_split(\n",
        "                X, y, test_size=0.2, random_state=42\n",
        "            )\n",
        "\n",
        "            # Create and train the pipeline\n",
        "            self.create_pipeline()\n",
        "            self.pipeline.fit(X_train, y_train)\n",
        "\n",
        "            # Evaluate on validation set\n",
        "            val_predictions = self.pipeline.predict(X_val)\n",
        "            mae = np.mean(np.abs(val_predictions - y_val))\n",
        "            print(f\"Validation MAE: {mae}\")\n",
        "\n",
        "            # Cross-validation for robustness\n",
        "            cv_scores = cross_val_score(\n",
        "                self.pipeline, X, y,\n",
        "                cv=5,\n",
        "                scoring='neg_mean_absolute_error'\n",
        "            )\n",
        "            print(f\"Cross-validation MAE: {-np.mean(cv_scores):.2f} ± {np.std(cv_scores):.2f}\")\n",
        "\n",
        "            # Predict on test set\n",
        "            test_predictions = self.pipeline.predict(self.test[self.feature_columns])\n",
        "\n",
        "            # Create submission dataframe\n",
        "            submission = pd.DataFrame({\n",
        "                'ID': self.test['ID'],\n",
        "                'Total': test_predictions\n",
        "            })\n",
        "            submission.to_csv('predictions.csv', index=False)\n",
        "            print(\"Predictions saved to 'predictions.csv'\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pipeline = DiseasePredictionPipeline()\n",
        "    pipeline.run_pipeline()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ef185a1",
      "metadata": {
        "id": "5ef185a1"
      },
      "outputs": [],
      "source": [
        "# Second model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDHviuJ0dvft",
        "outputId": "e4cc4c9e-4c33-4e8f-a7fc-59bed301423c"
      },
      "id": "uDHviuJ0dvft",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.0.0)\n",
            "Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "0dbed84a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dbed84a",
        "outputId": "6a389a47-ebf9-4c4d-9b0b-770350f7ed56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in load_and_analyze_data: 'ID'\n",
            "An error occurred in run_pipeline: 'ID'\n",
            "Error in main: 'ID'\n"
          ]
        }
      ],
      "source": [
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.linear_model import HuberRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class EnhancedRegressorBase(BaseEstimator, RegressorMixin):\n",
        "    \"\"\"Base class for enhanced regressors with robust error handling and logging\"\"\"\n",
        "    def __init__(self, model_class, **params):\n",
        "        self.params = params\n",
        "        self.model = model_class(**self.params)\n",
        "        self.feature_importance_ = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        try:\n",
        "            self.model.fit(X, y)\n",
        "            if hasattr(self.model, 'feature_importances_'):\n",
        "                self.feature_importance_ = self.model.feature_importances_\n",
        "            return self\n",
        "        except Exception as e:\n",
        "            print(f\"Error during fitting: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def predict(self, X):\n",
        "        try:\n",
        "            return self.model.predict(X)\n",
        "        except Exception as e:\n",
        "            print(f\"Error during prediction: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {**{\"model_class\": self.model.__class__}, **self.params}\n",
        "\n",
        "    def set_params(self, **params):\n",
        "        for param, value in params.items():\n",
        "            if param == \"model_class\":\n",
        "                continue\n",
        "            self.params[param] = value\n",
        "        self.model.set_params(**self.params)\n",
        "        return self\n",
        "\n",
        "class EnhancedXGBRegressor(EnhancedRegressorBase):\n",
        "    \"\"\"Enhanced XGBoost regressor with error handling\"\"\"\n",
        "    def __init__(self, **params):\n",
        "        super().__init__(XGBRegressor, **params)\n",
        "\n",
        "class EnhancedLGBMRegressor(EnhancedRegressorBase):\n",
        "    \"\"\"Enhanced LightGBM regressor with error handling\"\"\"\n",
        "    def __init__(self, **params):\n",
        "        super().__init__(LGBMRegressor, **params)\n",
        "\n",
        "class EnhancedCatBoostRegressor(EnhancedRegressorBase):\n",
        "    \"\"\"Enhanced CatBoost regressor with error handling\"\"\"\n",
        "    def __init__(self, **params):\n",
        "        super().__init__(CatBoostRegressor, **params)\n",
        "\n",
        "class ImprovedDiseasePredictionPipeline:\n",
        "    \"\"\"Main pipeline class for disease prediction\"\"\"\n",
        "    def __init__(self):\n",
        "        self.train = None\n",
        "        self.test = None\n",
        "        self.pipeline = None\n",
        "        self.feature_columns = None\n",
        "        self.numeric_features = None\n",
        "        self.categorical_features = None\n",
        "        self.feature_importance = None\n",
        "        self.model_metrics = {}\n",
        "        self.target_transformed = False\n",
        "\n",
        "    def load_and_analyze_data(self, waste_path=\"waste_management.csv\",\n",
        "                              toilet_path=\"toilet.csv\", water_path=\"water_sources.csv\"):\n",
        "        \"\"\"Load and perform initial statistical analysis of the data\"\"\"\n",
        "        try:\n",
        "            # Load datasets\n",
        "\n",
        "            self.test = pd.read_csv(waste_path)\n",
        "            self.test = pd.read_csv(toilet_path)\n",
        "            self.test = pd.read_csv(water_path)\n",
        "\n",
        "            # Statistical analysis\n",
        "            self._perform_statistical_analysis()\n",
        "\n",
        "            # Clean target variable\n",
        "            self._clean_target_variable()\n",
        "\n",
        "            # Define and verify features\n",
        "            self._setup_features()\n",
        "\n",
        "            # Handle missing and anomalous values\n",
        "            self._handle_data_quality()\n",
        "\n",
        "            return self.train, self.test\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in load_and_analyze_data: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _perform_statistical_analysis(self):\n",
        "        \"\"\"Perform comprehensive statistical analysis\"\"\"\n",
        "        try:\n",
        "            stats_report = {\n",
        "                'basic_stats': self.train.describe(),\n",
        "                'missing_values': self.train.isnull().sum(),\n",
        "                'skewness': self.train.select_dtypes(include=[np.number]).skew(),\n",
        "                'kurtosis': self.train.select_dtypes(include=[np.number]).kurtosis()\n",
        "            }\n",
        "\n",
        "            # Detect outliers using IQR method\n",
        "            numeric_cols = self.train.select_dtypes(include=[np.number]).columns\n",
        "            outliers_report = {}\n",
        "            for col in numeric_cols:\n",
        "                Q1 = self.train[col].quantile(0.25)\n",
        "                Q3 = self.train[col].quantile(0.75)\n",
        "                IQR = Q3 - Q1\n",
        "                outliers = ((self.train[col] < (Q1 - 1.5 * IQR)) |\n",
        "                           (self.train[col] > (Q3 + 1.5 * IQR))).sum()\n",
        "                outliers_report[col] = outliers\n",
        "\n",
        "            stats_report['outliers'] = outliers_report\n",
        "\n",
        "            # Print summary statistics\n",
        "            print(\"\\nData Analysis Summary:\")\n",
        "            print(f\"Total samples: {len(self.train)}\")\n",
        "            print(f\"Features: {len(self.train.columns)}\")\n",
        "            print(\"\\nMissing Values Summary:\")\n",
        "            print(stats_report['missing_values'])\n",
        "            print(\"\\nOutliers Summary:\")\n",
        "            print(stats_report['outliers'])\n",
        "\n",
        "            return stats_report\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in _perform_statistical_analysis: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _clean_target_variable(self):\n",
        "        \"\"\"Clean and transform target variable\"\"\"\n",
        "        try:\n",
        "            print(\"\\nTarget Variable Analysis:\")\n",
        "            print(self.train['Total'].describe())\n",
        "\n",
        "            # Remove invalid values (only remove extreme outliers)\n",
        "            original_len = len(self.train)\n",
        "            self.train = self.train[\n",
        "                (np.isfinite(self.train['Total'])) &\n",
        "                (self.train['Total'] >= 0) &\n",
        "                (self.train['Total'] <= self.train['Total'].quantile(0.995))  # Less aggressive outlier removal\n",
        "            ]\n",
        "\n",
        "            # Apply log transformation if highly skewed\n",
        "            if stats.skew(self.train['Total']) > 1:\n",
        "                self.train['Total'] = np.log1p(self.train['Total'])\n",
        "                self.target_transformed = True\n",
        "                print(\"\\nApplied log transformation to target variable due to high skewness\")\n",
        "\n",
        "            print(f\"\\nRemoved {original_len - len(self.train)} rows with invalid or extreme values\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in _clean_target_variable: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _setup_features(self):\n",
        "        \"\"\"Setup and verify features\"\"\"\n",
        "        try:\n",
        "            self.feature_columns = [col for col in self.train.columns\n",
        "                                  if col not in ['Total', 'ID']]\n",
        "\n",
        "            # Verify features\n",
        "            missing_cols = [col for col in self.feature_columns\n",
        "                           if col not in self.test.columns]\n",
        "            if missing_cols:\n",
        "                raise ValueError(f\"Missing columns in test dataset: {missing_cols}\")\n",
        "\n",
        "            # Analyze feature types\n",
        "            self.numeric_features = self.train[self.feature_columns].select_dtypes(\n",
        "                include=['float64', 'int64']).columns.tolist()\n",
        "            self.categorical_features = self.train[self.feature_columns].select_dtypes(\n",
        "                include=['object']).columns.tolist()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in _setup_features: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _handle_data_quality(self):\n",
        "        \"\"\"Handle missing values and anomalies\"\"\"\n",
        "        try:\n",
        "            for df in [self.train, self.test]:\n",
        "                # Handle numeric features\n",
        "                for col in self.numeric_features:\n",
        "                    # Replace infinite values\n",
        "                    df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "                    # Handle outliers using winsorization\n",
        "                    if col in df.columns:\n",
        "                        q1 = df[col].quantile(0.01)\n",
        "                        q3 = df[col].quantile(0.99)\n",
        "                        df[col] = df[col].clip(q1, q3)\n",
        "\n",
        "                # Handle categorical features\n",
        "                for col in self.categorical_features:\n",
        "                    if col in df.columns:\n",
        "                        # Convert rare categories to 'Other'\n",
        "                        value_counts = df[col].value_counts()\n",
        "                        rare_categories = value_counts[value_counts < len(df) * 0.01].index\n",
        "                        df[col] = df[col].replace(rare_categories, 'Other')\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in _handle_data_quality: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def create_advanced_pipeline(self):\n",
        "        \"\"\"Create an advanced pipeline with robust preprocessing and stacking\"\"\"\n",
        "        try:\n",
        "            # Numeric preprocessing\n",
        "            numeric_transformer = Pipeline(steps=[\n",
        "                ('imputer', KNNImputer(n_neighbors=5)),\n",
        "                ('scaler', RobustScaler()),\n",
        "                ('power', PowerTransformer(standardize=True))\n",
        "            ])\n",
        "\n",
        "            # Categorical preprocessing\n",
        "            categorical_transformer = Pipeline(steps=[\n",
        "                ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "                ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "            ])\n",
        "\n",
        "            # Create preprocessor\n",
        "            preprocessor = ColumnTransformer(\n",
        "                transformers=[\n",
        "                    ('num', numeric_transformer, self.numeric_features),\n",
        "                    ('cat', categorical_transformer, self.categorical_features)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            # Define base models with correct initialization\n",
        "            base_models = [\n",
        "                ('xgb', EnhancedXGBRegressor(\n",
        "                    n_estimators=200,\n",
        "                    learning_rate=0.05,\n",
        "                    max_depth=6,\n",
        "                    subsample=0.8,\n",
        "                    colsample_bytree=0.8,\n",
        "                    random_state=42\n",
        "                )),\n",
        "                ('lgbm', EnhancedLGBMRegressor(\n",
        "                    n_estimators=200,\n",
        "                    learning_rate=0.05,\n",
        "                    num_leaves=31,\n",
        "                    subsample=0.8,\n",
        "                    colsample_bytree=0.8,\n",
        "                    random_state=42\n",
        "                )),\n",
        "                ('catboost', EnhancedCatBoostRegressor(\n",
        "                    iterations=200,\n",
        "                    learning_rate=0.05,\n",
        "                    depth=6,\n",
        "                    subsample=0.8,\n",
        "                    random_state=42,\n",
        "                    verbose=False\n",
        "                ))\n",
        "            ]\n",
        "\n",
        "            # Create stacking model\n",
        "            final_estimator = HuberRegressor()\n",
        "            stacking = StackingRegressor(\n",
        "                estimators=base_models,\n",
        "                final_estimator=final_estimator,\n",
        "                cv=5,\n",
        "                n_jobs=-1\n",
        "            )\n",
        "\n",
        "            # Create final pipeline\n",
        "            self.pipeline = Pipeline(steps=[\n",
        "                ('preprocessor', preprocessor),\n",
        "                ('stacking', stacking)\n",
        "            ])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in create_advanced_pipeline: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def evaluate_model(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"Evaluate model performance with detailed metrics\"\"\"\n",
        "        try:\n",
        "            if X_val is not None and y_val is not None:\n",
        "                # Validation set metrics\n",
        "                val_pred = self.pipeline.predict(X_val)\n",
        "                self.model_metrics['validation'] = {\n",
        "                    'mae': mean_absolute_error(y_val, val_pred),\n",
        "                    'rmse': np.sqrt(mean_squared_error(y_val, val_pred)),\n",
        "                    'r2': r2_score(y_val, val_pred)\n",
        "                }\n",
        "\n",
        "            # Cross-validation metrics\n",
        "            cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "            cv_scores = cross_val_score(\n",
        "                self.pipeline, X, y,\n",
        "                cv=cv,\n",
        "                scoring='neg_mean_absolute_error',\n",
        "                n_jobs=-1\n",
        "            )\n",
        "\n",
        "            self.model_metrics['cross_validation'] = {\n",
        "                'mae_mean': -np.mean(cv_scores),\n",
        "                'mae_std': np.std(cv_scores)\n",
        "            }\n",
        "\n",
        "            # Print evaluation results\n",
        "            print(\"\\nModel Evaluation Results:\")\n",
        "            if 'validation' in self.model_metrics:\n",
        "                print(\"\\nValidation Metrics:\")\n",
        "                print(f\"MAE: {self.model_metrics['validation']['mae']:.4f}\")\n",
        "                print(f\"RMSE: {self.model_metrics['validation']['rmse']:.4f}\")\n",
        "                print(f\"R²: {self.model_metrics['validation']['r2']:.4f}\")\n",
        "\n",
        "            print(\"\\nCross-validation Metrics:\")\n",
        "            print(f\"MAE: {self.model_metrics['cross_validation']['mae_mean']:.4f} ± \"\n",
        "                  f\"{self.model_metrics['cross_validation']['mae_std']:.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in evaluate_model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def run_pipeline(self, train_path=\"Train.csv\", test_path=\"Test.csv\"):\n",
        "        \"\"\"Run the complete pipeline\"\"\"\n",
        "        try:\n",
        "            # Load and analyze data\n",
        "            self.load_and_analyze_data(train_path, test_path)\n",
        "\n",
        "            # Prepare data\n",
        "            X = self.train[self.feature_columns]\n",
        "            y = self.train['Total']\n",
        "\n",
        "            # Split data\n",
        "            X_train, X_val, y_train, y_val = train_test_split(\n",
        "                X, y, test_size=0.2, random_state=42\n",
        "            )\n",
        "\n",
        "            # Create and train pipeline\n",
        "            self.create_advanced_pipeline()\n",
        "            self.pipeline.fit(X_train, y_train)\n",
        "\n",
        "            # Evaluate model\n",
        "            self.evaluate_model(X_train, y_train, X_val, y_val)\n",
        "\n",
        "            # Generate predictions\n",
        "            test_predictions = self.pipeline.predict(self.test[self.feature_columns])\n",
        "\n",
        "            # If target was log-transformed, reverse transform predictions\n",
        "            if self.target_transformed:\n",
        "                test_predictions = np.expm1(test_predictions)\n",
        "\n",
        "            # Create submission\n",
        "            submission = pd.DataFrame({\n",
        "                'ID': self.test['ID'],\n",
        "                'Total': test_predictions\n",
        "            })\n",
        "            submission.to_csv('predictions.csv', index=False)\n",
        "            print(\"\\nPredictions saved to 'predictions.csv'\")\n",
        "\n",
        "            return submission\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred in run_pipeline: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        pipeline = ImprovedDiseasePredictionPipeline()\n",
        "        pipeline.run_pipeline()\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# updating it model 3"
      ],
      "metadata": {
        "id": "BN-k5VY8jikM"
      },
      "id": "BN-k5VY8jikM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost==1.7.5\n",
        "!pip install scikit-learn==1.2.2"
      ],
      "metadata": {
        "id": "hyDwe_yyPmQe",
        "outputId": "81578e41-1836-4588-b795-81e5c0c28f5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 733
        }
      },
      "id": "hyDwe_yyPmQe",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xgboost==1.7.5\n",
            "  Downloading xgboost-1.7.5-py3-none-manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost==1.7.5) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost==1.7.5) (1.13.1)\n",
            "Downloading xgboost-1.7.5-py3-none-manylinux2014_x86_64.whl (200.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.3/200.3 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xgboost\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 2.1.3\n",
            "    Uninstalling xgboost-2.1.3:\n",
            "      Successfully uninstalled xgboost-2.1.3\n",
            "Successfully installed xgboost-1.7.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "xgboost"
                ]
              },
              "id": "f6eb8d718df64f74961c8c39b7c40d65"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==1.2.2\n",
            "  Downloading scikit_learn-1.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (3.5.0)\n",
            "Downloading scikit_learn-1.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "mlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed scikit-learn-1.2.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sklearn"
                ]
              },
              "id": "bbff458be070467eaf96717ebbea2d71"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial import cKDTree\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.ensemble import VotingRegressor\n",
        "\n",
        "# Load datasets\n",
        "train = pd.read_csv(\"Train.csv\")\n",
        "test = pd.read_csv(\"Test.csv\")\n",
        "toilets = pd.read_csv(\"toilets.csv\")\n",
        "waste_management = pd.read_csv(\"waste_management.csv\")\n",
        "water_sources = pd.read_csv(\"water_sources.csv\")\n",
        "\n",
        "# Combine train and test datasets for consistent preprocessing\n",
        "hospital_data = pd.concat([train, test])\n",
        "\n",
        "# Drop unnecessary columns from supplementary datasets\n",
        "for df in [toilets, waste_management, water_sources]:\n",
        "    df.drop(columns=['Year', 'Month'], inplace=True)\n",
        "\n",
        "# Rename columns for clarity\n",
        "def rename_columns(df, prefix):\n",
        "    for col in df.columns:\n",
        "        if col not in ['Month_Year_lat_lon', 'lat_lon']:\n",
        "            df.rename(columns={col: f\"{prefix}_{col}\"}, inplace=True)\n",
        "\n",
        "rename_columns(toilets, \"toilet\")\n",
        "rename_columns(waste_management, \"waste\")\n",
        "rename_columns(water_sources, \"water\")\n",
        "\n",
        "# Ensure unique identifier columns exist in all supplementary datasets\n",
        "\n",
        "\n",
        "\n",
        "# Fill missing values in the 'Total' column\n",
        "hospital_data['Total'].fillna(0, inplace=True)\n",
        "\n",
        "# Drop rows with missing latitude and longitude in water sources\n",
        "water_sources.dropna(subset=['water_Transformed_Latitude'], inplace=True)\n",
        "\n",
        "# Function to find nearest locations\n",
        "def find_nearest(hospital_data, location_df, lat_col, lon_col, id_col):\n",
        "    # Create a cKDTree for efficient nearest neighbour search\n",
        "    tree = cKDTree(location_df[[lat_col, lon_col]].values)\n",
        "    nearest = {}\n",
        "    # Loop through each hospital and find the nearest site in location_df\n",
        "    for _, row in hospital_data.iterrows():\n",
        "        _, idx = tree.query([row['Transformed_Latitude'], row['Transformed_Longitude']])\n",
        "        nearest[row['ID']] = location_df.iloc[idx][id_col]\n",
        "    return nearest\n",
        "\n",
        "for df, prefix in [(toilets, 'toilet'), (waste_management, 'waste'), (water_sources, 'water')]:\n",
        "   df[f\"{prefix}_Month_Year_lat_lon\"] = (\n",
        "      df[f\"{prefix}_Month_Year\"] + '_' +\n",
        "      df[f\"{prefix}_Transformed_Latitude\"].astype(str) + '_' +\n",
        "      df[f\"{prefix}_Transformed_Longitude\"].astype(str)\n",
        "    )\n",
        "\n",
        "\n",
        "# Merge datasets with nearest locations\n",
        "merged_data = hospital_data.copy()\n",
        "datasets = [\n",
        "    (toilets, 'toilet', 'toilet_Month_Year_lat_lon'),\n",
        "    (waste_management, 'waste', 'waste_Month_Year_lat_lon'),\n",
        "    (water_sources, 'water', 'water_Month_Year_lat_lon'),\n",
        "]\n",
        "\n",
        "for df, prefix, id_col in datasets:\n",
        "    nearest = find_nearest(merged_data, df, f\"{prefix}_Transformed_Latitude\", f\"{prefix}_Transformed_Longitude\", id_col)\n",
        "    nearest_df = pd.DataFrame(list(nearest.items()), columns=['ID', id_col])\n",
        "    merged_data = merged_data.merge(nearest_df, on=\"ID\").merge(df, on=id_col)\n",
        "\n",
        "# Split merged data into train and test sets\n",
        "train_df = merged_data[merged_data['Year'] < 2023]\n",
        "test_df = merged_data[merged_data['Year'] == 2023]\n",
        "\n",
        "# Specify the target column\n",
        "target_column = 'Total'\n",
        "\n",
        "# Feature and target split\n",
        "X = train_df.drop(columns=[target_column, 'ID', 'Location'], errors='ignore')\n",
        "y = train_df[target_column]\n",
        "\n",
        "# Handle categorical features\n",
        "categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col])\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Hyperparameter tuning for Random Forest\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "rf_params = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "}\n",
        "rf_grid = GridSearchCV(rf, rf_params, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "rf_grid.fit(X_train, y_train)\n",
        "best_rf = rf_grid.best_estimator_\n",
        "\n",
        "# Hyperparameter tuning for XGBoost\n",
        "xgb = XGBRegressor(random_state=42, verbosity=0)\n",
        "xgb_params = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [3, 6],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "    'subsample': [0.8, 1.0],\n",
        "}\n",
        "xgb_grid = GridSearchCV(xgb, xgb_params, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "xgb_grid.fit(X_train, y_train)\n",
        "best_xgb = xgb_grid.best_estimator_\n",
        "\n",
        "# Create a hybrid model using Voting Regressor\n",
        "hybrid_model = VotingRegressor([('RandomForest', best_rf), ('XGBoost', best_xgb)])\n",
        "hybrid_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = hybrid_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "\n",
        "# Prepare test data\n",
        "X_test_final = test_df.drop(columns=['Total', 'ID', 'Location'], errors='ignore')\n",
        "\n",
        "# Handle categorical features in test data\n",
        "for col in categorical_cols:\n",
        "    if col in X_test_final.columns:\n",
        "        le = LabelEncoder()\n",
        "        X_test_final[col] = le.fit_transform(X_test_final[col])\n",
        "\n",
        "# Align test dataset with training features\n",
        "for col in X.columns:\n",
        "    if col not in X_test_final.columns:\n",
        "        X_test_final[col] = 0  # Add missing feature with default value (e.g., zero)\n",
        "\n",
        "# Ensure columns are in the same order as training\n",
        "X_test_final = X_test_final[X.columns]\n",
        "\n",
        "# Make predictions on test data\n",
        "predictions = hybrid_model.predict(X_test_final)\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "test_predictions = pd.DataFrame({\n",
        "    'ID': test_df['ID'],\n",
        "    'Predictions': predictions\n",
        "})\n",
        "\n",
        "test_predictions.to_csv(\"test_predictions.csv\", index=False)\n",
        "print(\"Predictions saved to 'test_predictions.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "96C5dAzqjlb0",
        "outputId": "a5fa0fac-e5c1-4cc4-fb16-89dec7d42694"
      },
      "id": "96C5dAzqjlb0",
      "execution_count": 1,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-1-06f9426cbc46>:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  hospital_data['Total'].fillna(0, inplace=True)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-06f9426cbc46>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m }\n\u001b[1;32m    107\u001b[0m \u001b[0mrf_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'neg_mean_absolute_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m \u001b[0mrf_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0mbest_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1761\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MLflow"
      ],
      "metadata": {
        "id": "ra5PDEQ1iVkf"
      },
      "id": "ra5PDEQ1iVkf",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlflow scikit-fuzzy deap xgboost"
      ],
      "metadata": {
        "id": "hRSx7POGi14F",
        "outputId": "863ba816-e87f-4708-b4b3-6141b6067fe5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "hRSx7POGi14F",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mlflow\n",
            "  Downloading mlflow-2.19.0-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting scikit-fuzzy\n",
            "  Downloading scikit_fuzzy-0.5.0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting deap\n",
            "  Downloading deap-1.4.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (1.7.5)\n",
            "Collecting mlflow-skinny==2.19.0 (from mlflow)\n",
            "  Downloading mlflow_skinny-2.19.0-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.0)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.5)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
            "  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting graphene<4 (from mlflow)\n",
            "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting gunicorn<24 (from mlflow)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.7)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.26.4)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<19,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (17.0.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.2.2)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.13.1)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.37)\n",
            "Requirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.19.0->mlflow) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.19.0->mlflow) (8.1.8)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.19.0->mlflow) (3.1.0)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.19.0->mlflow)\n",
            "  Downloading databricks_sdk-0.40.0-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.19.0->mlflow) (3.1.44)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.19.0->mlflow) (8.5.0)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.19.0->mlflow) (1.29.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.19.0->mlflow) (1.29.0)\n",
            "Requirement already satisfied: packaging<25 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.19.0->mlflow) (24.2)\n",
            "Requirement already satisfied: protobuf<6,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.19.0->mlflow) (4.25.5)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.19.0->mlflow) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.19.0->mlflow) (2.32.3)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.19.0->mlflow) (0.5.3)\n",
            "Collecting Mako (from alembic!=1.10.0,<2->mlflow)\n",
            "  Downloading Mako-1.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.11/dist-packages (from alembic!=1.10.0,<2->mlflow) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.3.0)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_core-3.2.5-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2<4,>=2.11->mlflow) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (3.2.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (3.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.1)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (2.27.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.19.0->mlflow) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.19.0->mlflow) (3.21.0)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.19.0->mlflow) (1.2.15)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.50b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.19.0->mlflow) (0.50b0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (2024.12.14)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.19.0->mlflow) (1.17.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.19.0->mlflow) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (0.6.1)\n",
            "Downloading mlflow-2.19.0-py3-none-any.whl (27.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.4/27.4 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-2.19.0-py3-none-any.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_fuzzy-0.5.0-py2.py3-none-any.whl (920 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m920.8/920.8 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deap-1.4.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading databricks_sdk-0.40.0-py3-none-any.whl (629 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m629.7/629.7 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.5-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading Mako-1.3.8-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-fuzzy, Mako, gunicorn, graphql-core, deap, graphql-relay, docker, alembic, graphene, databricks-sdk, mlflow-skinny, mlflow\n",
            "Successfully installed Mako-1.3.8 alembic-1.14.0 databricks-sdk-0.40.0 deap-1.4.2 docker-7.1.0 graphene-3.4.3 graphql-core-3.2.5 graphql-relay-3.2.0 gunicorn-23.0.0 mlflow-2.19.0 mlflow-skinny-2.19.0 scikit-fuzzy-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from scipy.spatial import cKDTree\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import VotingRegressor\n",
        "from sklearn.impute import KNNImputer\n",
        "import skfuzzy as fuzz\n",
        "from deap import base, creator, tools, algorithms\n",
        "import warnings\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "class ANFISLayer:\n",
        "    def __init__(self, n_inputs, n_rules, learning_rate=0.01):\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_rules = n_rules\n",
        "        self.learning_rate = learning_rate\n",
        "        self.membership_params = np.random.randn(n_inputs, n_rules, 3)  # center, width, type\n",
        "        self.consequence_params = np.random.randn(n_rules, n_inputs + 1)\n",
        "\n",
        "    def membership_function(self, x, params):\n",
        "        center, width, mf_type = params\n",
        "        if mf_type > 0.5:  # Gaussian\n",
        "            return np.array(fuzz.gaussmf(x, center, width))\n",
        "        else:  # Bell-shaped\n",
        "            return np.array(fuzz.gbellmf(x, width, 2, center))\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Layer 1: Fuzzification\n",
        "        membership_values = np.zeros((X.shape[0], self.n_inputs, self.n_rules))\n",
        "        for i in range(self.n_inputs):\n",
        "            for j in range(self.n_rules):\n",
        "                membership_values[:, i, j] = self.membership_function(X[:, i],\n",
        "                                                                    self.membership_params[i, j])\n",
        "\n",
        "        # Layer 2: Rules\n",
        "        rule_outputs = np.prod(membership_values, axis=1)\n",
        "\n",
        "        # Layer 3: Normalization\n",
        "        normalized_firing_strengths = rule_outputs / (np.sum(rule_outputs, axis=1, keepdims=True) + 1e-10)\n",
        "\n",
        "        # Layer 4: Consequence\n",
        "        extended_X = np.column_stack([X, np.ones(X.shape[0])])\n",
        "        consequent_outputs = np.dot(extended_X, self.consequence_params.T)\n",
        "\n",
        "        # Layer 5: Output\n",
        "        final_output = np.sum(normalized_firing_strengths * consequent_outputs, axis=1)\n",
        "        return final_output\n",
        "\n",
        "    def backward(self, X, y, y_pred):\n",
        "        # Gradient descent update for consequence parameters\n",
        "        extended_X = np.column_stack([X, np.ones(X.shape[0])])\n",
        "        error = y - y_pred\n",
        "\n",
        "        # Update consequence parameters\n",
        "        for i in range(self.n_rules):\n",
        "            gradient = -2 * error * extended_X\n",
        "            self.consequence_params[i] -= self.learning_rate * np.mean(gradient, axis=0)\n",
        "\n",
        "        # Update membership parameters (simplified)\n",
        "        for i in range(self.n_inputs):\n",
        "            for j in range(self.n_rules):\n",
        "                gradient = -2 * error * (y_pred - np.mean(y_pred))\n",
        "                self.membership_params[i, j, 0] -= self.learning_rate * np.mean(gradient)  # center\n",
        "                self.membership_params[i, j, 1] -= self.learning_rate * np.mean(gradient)  # width\n",
        "\n",
        "class GeneticOptimizer:\n",
        "    def __init__(self, population_size=50, n_generations=30):\n",
        "        self.population_size = population_size\n",
        "        self.n_generations = n_generations\n",
        "        creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
        "        creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
        "\n",
        "    def optimize(self, anfis, X, y):\n",
        "        toolbox = base.Toolbox()\n",
        "        n_params = np.prod(anfis.membership_params.shape) + np.prod(anfis.consequence_params.shape)\n",
        "\n",
        "        toolbox.register(\"attr_float\", np.random.uniform, -1, 1)\n",
        "        toolbox.register(\"individual\", tools.initRepeat, creator.Individual,\n",
        "                        toolbox.attr_float, n=n_params)\n",
        "        toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "\n",
        "        def evaluate(individual):\n",
        "            # Update ANFIS parameters\n",
        "            membership_params = np.array(individual[:np.prod(anfis.membership_params.shape)])\n",
        "            consequence_params = np.array(individual[np.prod(anfis.membership_params.shape):])\n",
        "            anfis.membership_params = membership_params.reshape(anfis.membership_params.shape)\n",
        "            anfis.consequence_params = consequence_params.reshape(anfis.consequence_params.shape)\n",
        "\n",
        "            # Calculate fitness\n",
        "            predictions = anfis.forward(X)\n",
        "            return (mean_absolute_error(y, predictions),)\n",
        "\n",
        "        toolbox.register(\"evaluate\", evaluate)\n",
        "        toolbox.register(\"mate\", tools.cxTwoPoint)\n",
        "        toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=0.1, indpb=0.1)\n",
        "        toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
        "\n",
        "        # Evolution\n",
        "        population = toolbox.population(n=self.population_size)\n",
        "        algorithms.eaSimple(population, toolbox, cxpb=0.7, mutpb=0.3,\n",
        "                          ngen=self.n_generations, verbose=False)\n",
        "\n",
        "        # Return best individual\n",
        "        return tools.selBest(population, k=1)[0]\n",
        "\n",
        "class ImprovedDiseasePredictionPipeline:\n",
        "    def __init__(self, experiment_name=\"disease_prediction\", target_column=\"Disease_Cases\"):\n",
        "        self.experiment_name = experiment_name\n",
        "        self.target_column = target_column\n",
        "        mlflow.set_experiment(experiment_name)\n",
        "        self.models = {}\n",
        "        self.scalers = {}\n",
        "        self.label_encoders = {}\n",
        "        self.anfis = None\n",
        "        self.best_genetic_params = None\n",
        "\n",
        "    def load_and_preprocess_data(self, train_path, test_path, toilets_path,\n",
        "                                waste_path, water_path):\n",
        "        with mlflow.start_run(run_name=\"data_preprocessing\"):\n",
        "            # Load datasets\n",
        "            try:\n",
        "                train = pd.read_csv(train_path)\n",
        "                test = pd.read_csv(test_path)\n",
        "                toilets = pd.read_csv(toilets_path)\n",
        "                waste = pd.read_csv(waste_path)\n",
        "                water = pd.read_csv(water_path)\n",
        "            except FileNotFoundError as e:\n",
        "                logging.error(f\"Error loading data: {str(e)}\")\n",
        "                raise\n",
        "\n",
        "            # Verify target column exists\n",
        "            if self.target_column not in train.columns:\n",
        "                raise ValueError(f\"Target column '{self.target_column}' not found in training data\")\n",
        "\n",
        "            # Log data statistics\n",
        "            mlflow.log_param(\"train_shape\", train.shape)\n",
        "            mlflow.log_param(\"test_shape\", test.shape)\n",
        "\n",
        "            # Clean and preprocess data\n",
        "            for df in [toilets, waste, water]:\n",
        "                df.drop(columns=['Year', 'Month'], inplace=True, errors='ignore')\n",
        "\n",
        "            # Feature engineering\n",
        "            self._create_temporal_features(train)\n",
        "            self._create_temporal_features(test)\n",
        "            self._create_spatial_features(train, [toilets, waste, water])\n",
        "            self._create_spatial_features(test, [toilets, waste, water])\n",
        "\n",
        "            # Handle missing values\n",
        "            imputer = KNNImputer(n_neighbors=5)\n",
        "            numeric_columns = train.select_dtypes(include=[np.number]).columns\n",
        "            train[numeric_columns] = imputer.fit_transform(train[numeric_columns])\n",
        "            test[numeric_columns] = imputer.transform(test[numeric_columns])\n",
        "\n",
        "            # Scale features\n",
        "            self.scalers['standard'] = StandardScaler()\n",
        "            self.scalers['robust'] = RobustScaler()\n",
        "\n",
        "            scaled_train = train.copy()\n",
        "            scaled_test = test.copy()\n",
        "\n",
        "            scaled_train[numeric_columns] = self.scalers['standard'].fit_transform(train[numeric_columns])\n",
        "            scaled_test[numeric_columns] = self.scalers['standard'].transform(test[numeric_columns])\n",
        "\n",
        "            # Encode categorical variables\n",
        "            categorical_columns = train.select_dtypes(include=['object']).columns\n",
        "            for col in categorical_columns:\n",
        "                self.label_encoders[col] = LabelEncoder()\n",
        "                scaled_train[col] = self.label_encoders[col].fit_transform(train[col])\n",
        "                scaled_test[col] = self.label_encoders[col].transform(test[col])\n",
        "\n",
        "            return scaled_train, scaled_test\n",
        "\n",
        "    def _create_temporal_features(self, df):\n",
        "        if 'Date' in df.columns:\n",
        "            df['Date'] = pd.to_datetime(df['Date'])\n",
        "            df['Month'] = df['Date'].dt.month\n",
        "            df['Season'] = df['Date'].dt.month % 12 // 3 + 1\n",
        "            df['DayOfYear'] = df['Date'].dt.dayofyear\n",
        "            df['WeekOfYear'] = df['Date'].dt.isocalendar().week\n",
        "            df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "\n",
        "    def _create_spatial_features(self, df, auxiliary_dfs):\n",
        "        for i, aux_df in enumerate(auxiliary_dfs):\n",
        "            if 'Latitude' in aux_df.columns and 'Longitude' in aux_df.columns:\n",
        "                tree = cKDTree(aux_df[['Latitude', 'Longitude']].values)\n",
        "                distances, indices = tree.query(df[['Latitude', 'Longitude']].values, k=3)\n",
        "\n",
        "                # Distance-based features\n",
        "                df[f'aux_{i}_nearest_dist'] = distances[:, 0]\n",
        "                df[f'aux_{i}_avg_3_nearest'] = distances.mean(axis=1)\n",
        "                df[f'aux_{i}_max_3_nearest'] = distances.max(axis=1)\n",
        "\n",
        "                # Density-based features\n",
        "                radius = np.percentile(distances[:, 0], 75)  # 75th percentile radius\n",
        "                neighbors = tree.query_ball_point(df[['Latitude', 'Longitude']].values, radius)\n",
        "                df[f'aux_{i}_density'] = [len(n) for n in neighbors]\n",
        "\n",
        "    def train_hybrid_model(self, X_train, y_train, X_val, y_val):\n",
        "        with mlflow.start_run(run_name=\"model_training\"):\n",
        "            # Initialize and train base models\n",
        "            self.models['rf'] = RandomForestRegressor(\n",
        "                n_estimators=200,\n",
        "                max_depth=15,\n",
        "                min_samples_split=5,\n",
        "                min_samples_leaf=2,\n",
        "                n_jobs=-1,\n",
        "                random_state=42\n",
        "            )\n",
        "\n",
        "            self.models['xgb'] = XGBRegressor(\n",
        "                n_estimators=200,\n",
        "                learning_rate=0.05,\n",
        "                max_depth=6,\n",
        "                min_child_weight=2,\n",
        "                subsample=0.8,\n",
        "                colsample_bytree=0.8,\n",
        "                random_state=42\n",
        "            )\n",
        "\n",
        "            self.models['gbm'] = GradientBoostingRegressor(\n",
        "                n_estimators=200,\n",
        "                learning_rate=0.05,\n",
        "                max_depth=4,\n",
        "                min_samples_split=5,\n",
        "                min_samples_leaf=2,\n",
        "                random_state=42\n",
        "            )\n",
        "\n",
        "            # Train base models\n",
        "            for name, model in self.models.items():\n",
        "                logging.info(f\"Training {name} model...\")\n",
        "                model.fit(X_train, y_train)\n",
        "                val_pred = model.predict(X_val)\n",
        "                mae = mean_absolute_error(y_val, val_pred)\n",
        "                mlflow.log_metric(f\"{name}_mae\", mae)\n",
        "\n",
        "            # Initialize and train ANFIS-GA hybrid\n",
        "            logging.info(\"Training ANFIS-GA hybrid model...\")\n",
        "            self.anfis = ANFISLayer(n_inputs=X_train.shape[1], n_rules=5)\n",
        "            genetic_optimizer = GeneticOptimizer(population_size=50, n_generations=30)\n",
        "            self.best_genetic_params = genetic_optimizer.optimize(self.anfis, X_train, y_train)\n",
        "\n",
        "            # Fine-tune ANFIS with backpropagation\n",
        "            n_epochs = 50\n",
        "            for epoch in range(n_epochs):\n",
        "                predictions = self.anfis.forward(X_train)\n",
        "                self.anfis.backward(X_train, y_train, predictions)\n",
        "\n",
        "                if epoch % 10 == 0:\n",
        "                    val_predictions = self.anfis.forward(X_val)\n",
        "                    val_mae = mean_absolute_error(y_val, val_predictions)\n",
        "                    logging.info(f\"Epoch {epoch}: Validation MAE = {val_mae:.4f}\")\n",
        "\n",
        "            # Create ensemble predictions\n",
        "            ensemble_predictions = np.zeros(len(X_val))\n",
        "            for model in self.models.values():\n",
        "                ensemble_predictions += model.predict(X_val)\n",
        "            anfis_predictions = self.anfis.forward(X_val)\n",
        "\n",
        "            # Weighted combination with optimal weights\n",
        "            weights = np.array([0.3, 0.3, 0.1, 0.3])  # RF, XGB, GBM, ANFIS\n",
        "            final_predictions = (\n",
        "                weights[0] * self.models['rf'].predict(X_val) +\n",
        "                weights[1] * self.models['xgb'].predict(X_val) +\n",
        "                weights[2] * self.models['gbm'].predict(X_val) +\n",
        "                weights[3] * self.anfis.forward(X_val)\n",
        "            )\n",
        "\n",
        "            # Calculate and log metrics\n",
        "            mae = mean_absolute_error(y_val, final_predictions)\n",
        "            rmse = np.sqrt(mean_squared_error(y_val, final_predictions))\n",
        "            r2 = r2_score(y_val, final_predictions)\n",
        "\n",
        "            mlflow.log_metrics({\n",
        "                \"final_mae\": mae,\n",
        "                \"final_rmse\": rmse,\n",
        "                \"final_r2\": r2\n",
        "            })\n",
        "\n",
        "            logging.info(f\"Final Validation Metrics:\")\n",
        "            logging.info(f\"MAE: {mae:.4f}\")\n",
        "            logging.info(f\"RMSE: {rmse:.4f}\")\n",
        "            logging.info(f\"R2: {r2:.4f}\")\n",
        "\n",
        "            return mae, rmse, r2\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        # Ensemble predictions with optimal weights\n",
        "        weights = np.array([0.3, 0.3, 0.1, 0.3])  # RF, XGB, GBM, ANFIS\n",
        "        final_predictions = (\n",
        "            weights[0] * self.models['rf'].predict(X_test) +\n",
        "            weights[1] * self.models['xgb'].predict(X_test) +\n",
        "            weights[2] * self.models['gbm'].predict(X_test) +\n",
        "            weights[3] * self.anfis.forward(X_test)\n",
        "        )\n",
        "        return final_predictions\n",
        "\n",
        "    def save_predictions(self, predictions, ids, output_path):\n",
        "        \"\"\"Save predictions to CSV and log as MLflow artifact\"\"\"\n",
        "        try:\n",
        "            submission = pd.DataFrame({\n",
        "                'ID': ids,\n",
        "                'Predicted_Cases': predictions\n",
        "            })\n",
        "            submission.to_csv(output_path, index=False)\n",
        "            mlflow.log_artifact(output_path)\n",
        "            logging.info(f\"Predictions saved to {output_path}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error saving predictions: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def save_model(self, output_dir):\n",
        "        \"\"\"Save all models and parameters\"\"\"\n",
        "        try:\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "            # Save sklearn models\n",
        "            for name, model in self.models.items():\n",
        "                model_path = os.path.join(output_dir, f\"{name}_model.pkl\")\n",
        "                mlflow.sklearn.save_model(model, model_path)\n",
        "\n",
        "            # Save ANFIS parameters\n",
        "            anfis_params = {\n",
        "                'membership_params': self.anfis.membership_params,\n",
        "                'consequence_params': self.anfis.consequence_params,\n",
        "                'best_genetic_params': self.best_genetic_params\n",
        "            }\n",
        "            np.save(os.path.join(output_dir, \"anfis_params.npy\"), anfis_params)\n",
        "\n",
        "            # Save scalers and encoders\n",
        "            for name, scaler in self.scalers.items():\n",
        "                mlflow.sklearn.save_model(scaler, os.path.join(output_dir, f\"{name}_scaler.pkl\"))\n",
        "\n",
        "            for col, encoder in self.label_encoders.items():\n",
        "                mlflow.sklearn.save_model(encoder, os.path.join(output_dir, f\"{col}_encoder.pkl\"))\n",
        "\n",
        "            logging.info(f\"Models and parameters saved to {output_dir}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error saving models: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def load_model(self, input_dir):\n",
        "        \"\"\"Load all models and parameters\"\"\"\n",
        "        try:\n",
        "            # Load sklearn models\n",
        "            for name in ['rf', 'xgb', 'gbm']:\n",
        "                model_path = os.path.join(input_dir, f\"{name}_model.pkl\")\n",
        "                self.models[name] = mlflow.sklearn.load_model(model_path)\n",
        "\n",
        "            # Load ANFIS parameters\n",
        "            anfis_params = np.load(os.path.join(input_dir, \"anfis_params.npy\"), allow_pickle=True).item()\n",
        "            self.anfis = ANFISLayer(\n",
        "                n_inputs=anfis_params['membership_params'].shape[0],\n",
        "                n_rules=anfis_params['membership_params'].shape[1]\n",
        "            )\n",
        "            self.anfis.membership_params = anfis_params['membership_params']\n",
        "            self.anfis.consequence_params = anfis_params['consequence_params']\n",
        "            self.best_genetic_params = anfis_params['best_genetic_params']\n",
        "\n",
        "            # Load scalers and encoders\n",
        "            for name in ['standard', 'robust']:\n",
        "                self.scalers[name] = mlflow.sklearn.load_model(os.path.join(input_dir, f\"{name}_scaler.pkl\"))\n",
        "\n",
        "            # Load any existing label encoders\n",
        "            encoder_files = [f for f in os.listdir(input_dir) if f.endswith('_encoder.pkl')]\n",
        "            for file in encoder_files:\n",
        "                col = file.replace('_encoder.pkl', '')\n",
        "                self.label_encoders[col] = mlflow.sklearn.load_model(os.path.join(input_dir, file))\n",
        "\n",
        "            logging.info(f\"Models and parameters loaded from {input_dir}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error loading models: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the disease prediction pipeline\"\"\"\n",
        "    try:\n",
        "        # Set up logging\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "            handlers=[\n",
        "                logging.FileHandler('disease_prediction.log'),\n",
        "                logging.StreamHandler()\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Initialize pipeline\n",
        "        pipeline = ImprovedDiseasePredictionPipeline(\n",
        "            experiment_name=\"disease_prediction_experiment\",\n",
        "            target_column=\"Total\"  # Adjust based on your data\n",
        "        )\n",
        "\n",
        "        # Define data paths\n",
        "        data_paths = {\n",
        "            'train': \"Train.csv\",\n",
        "            'test': \"Test.csv\",\n",
        "            'toilets': \"toilets.csv\",\n",
        "            'waste': \"waste_management.csv\",\n",
        "            'water': \"water_sources.csv\"\n",
        "        }\n",
        "\n",
        "        # Load and preprocess data\n",
        "        logging.info(\"Loading and preprocessing data...\")\n",
        "        train, test = pipeline.load_and_preprocess_data(\n",
        "            data_paths['train'],\n",
        "            data_paths['test'],\n",
        "            data_paths['toilets'],\n",
        "            data_paths['waste'],\n",
        "            data_paths['water']\n",
        "        )\n",
        "\n",
        "        # Split data for training\n",
        "        logging.info(\"Splitting data for training...\")\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            train.drop(['ID', pipeline.target_column], axis=1),\n",
        "            train[pipeline.target_column],\n",
        "            test_size=0.2,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        # Train model\n",
        "        logging.info(\"Training hybrid model...\")\n",
        "        mae, rmse, r2 = pipeline.train_hybrid_model(X_train, y_train, X_val, y_val)\n",
        "\n",
        "        # Generate predictions for test set\n",
        "        logging.info(\"Generating predictions for test set...\")\n",
        "        X_test = test.drop(['ID'], axis=1)\n",
        "        predictions = pipeline.predict(X_test)\n",
        "\n",
        "        # Save predictions\n",
        "        output_path = f\"predictions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "        pipeline.save_predictions(predictions, test['ID'], output_path)\n",
        "\n",
        "        # Save models\n",
        "        pipeline.save_model(\"models\")\n",
        "\n",
        "        logging.info(\"Pipeline execution completed successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Pipeline execution failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "TRA3lrJoih9y",
        "outputId": "0ceb7bcc-af57-47dc-abb1-d69ce9512f5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "id": "TRA3lrJoih9y",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Pipeline execution failed: \"['Total'] not in index\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['Total'] not in index\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-01ac797bdd8a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-01ac797bdd8a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;31m# Load and preprocess data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading and preprocessing data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m         train, test = pipeline.load_and_preprocess_data(\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0mdata_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0mdata_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-01ac797bdd8a>\u001b[0m in \u001b[0;36mload_and_preprocess_data\u001b[0;34m(self, train_path, test_path, toilets_path, waste_path, water_path)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mnumeric_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumeric_columns\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimputer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumeric_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumeric_columns\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimputer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumeric_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;31m# Scale features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6251\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6252\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6254\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['Total'] not in index\""
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}