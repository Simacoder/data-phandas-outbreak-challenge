{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Simacoder/data-phandas-outbreak-challenge/blob/main/Model7%20updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c70b70c0",
      "metadata": {
        "id": "c70b70c0"
      },
      "source": [
        "# Model7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b95ce335",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b95ce335",
        "outputId": "39e18c13-c5b2-4017-f628-43c8c8087ef4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in training dataset: Index(['ID', 'Total', 'Location', 'Category_Health_Facility_UUID', 'Disease',\n",
            "       'Month', 'Year', 'Transformed_Latitude', 'Transformed_Longitude'],\n",
            "      dtype='object')\n",
            "Columns in testing dataset: Index(['Location', 'Disease', 'Month', 'Category_Health_Facility_UUID', 'Year',\n",
            "       'Transformed_Latitude', 'Transformed_Longitude', 'ID'],\n",
            "      dtype='object')\n",
            "\n",
            "Initial target variable statistics:\n",
            "count    23847.000000\n",
            "mean         8.355600\n",
            "std         28.076713\n",
            "min          0.000000\n",
            "25%          0.000000\n",
            "50%          0.000000\n",
            "75%          3.000000\n",
            "max        489.000000\n",
            "Name: Total, dtype: float64\n",
            "\n",
            "Target variable statistics after cleaning:\n",
            "count    23847.000000\n",
            "mean         8.355600\n",
            "std         28.076713\n",
            "min          0.000000\n",
            "25%          0.000000\n",
            "50%          0.000000\n",
            "75%          3.000000\n",
            "max        489.000000\n",
            "Name: Total, dtype: float64\n",
            "\n",
            "Removed 0 rows with invalid Total values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-796a3c6b0995>:81: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(df[col].median(), inplace=True)\n",
            "<ipython-input-2-796a3c6b0995>:87: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(df[col].mode()[0], inplace=True)\n",
            "<ipython-input-2-796a3c6b0995>:81: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(df[col].median(), inplace=True)\n",
            "<ipython-input-2-796a3c6b0995>:87: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(df[col].mode()[0], inplace=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation MAE: 9.90429589219813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation MAE: 11.14 Â± 2.23\n",
            "Predictions saved to 'predictions.csv'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Custom wrapper for XGBRegressor to ensure compatibility\n",
        "class CustomXGBRegressor(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, **params):\n",
        "        self.model = XGBRegressor(**params)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.model.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return self.model.get_params(deep)\n",
        "\n",
        "    def set_params(self, **params):\n",
        "        self.model.set_params(**params)\n",
        "        return self\n",
        "\n",
        "class DiseasePredictionPipeline:\n",
        "    def __init__(self):\n",
        "        self.train = None\n",
        "        self.test = None\n",
        "        self.pipeline = None\n",
        "        self.feature_columns = None\n",
        "\n",
        "    def load_data(self):\n",
        "        # Load datasets\n",
        "        self.train = pd.read_csv(\"Train.csv\")\n",
        "        self.test = pd.read_csv(\"Test.csv\")\n",
        "\n",
        "        print(\"Columns in training dataset:\", self.train.columns)\n",
        "        print(\"Columns in testing dataset:\", self.test.columns)\n",
        "\n",
        "        # Clean target variable\n",
        "        print(\"\\nInitial target variable statistics:\")\n",
        "        print(self.train['Total'].describe())\n",
        "\n",
        "        # Remove rows where Total is NaN, infinite, or extremely large\n",
        "        self.train = self.train[\n",
        "            np.isfinite(self.train['Total'])\n",
        "        ]\n",
        "\n",
        "        print(\"\\nTarget variable statistics after cleaning:\")\n",
        "        print(self.train['Total'].describe())\n",
        "        print(f\"\\nRemoved {len(self.train) - len(self.train)} rows with invalid Total values\")\n",
        "\n",
        "        # Define feature columns (excluding 'Total' and 'ID')\n",
        "        self.feature_columns = [col for col in self.train.columns\n",
        "                              if col not in ['Total', 'ID']]\n",
        "\n",
        "        # Verify all feature columns exist in test dataset\n",
        "        missing_cols = [col for col in self.feature_columns\n",
        "                       if col not in self.test.columns]\n",
        "        if missing_cols:\n",
        "            raise ValueError(f\"Missing columns in test dataset: {missing_cols}\")\n",
        "\n",
        "        # Handle missing values separately for train and test\n",
        "        self._handle_missing_values(self.train)\n",
        "        self._handle_missing_values(self.test)\n",
        "\n",
        "    def _handle_missing_values(self, df):\n",
        "        \"\"\"Handle missing values for a given dataframe\"\"\"\n",
        "        # Numerical columns\n",
        "        numerical_cols = df[self.feature_columns].select_dtypes(\n",
        "            include=['float64', 'int64']).columns\n",
        "        for col in numerical_cols:\n",
        "            # Replace infinite values with NaN first\n",
        "            df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
        "            # Then fill NaN with median\n",
        "            df[col].fillna(df[col].median(), inplace=True)\n",
        "\n",
        "        # Categorical columns\n",
        "        categorical_cols = df[self.feature_columns].select_dtypes(\n",
        "            include=['object']).columns\n",
        "        for col in categorical_cols:\n",
        "            df[col].fillna(df[col].mode()[0], inplace=True)\n",
        "\n",
        "    def create_pipeline(self):\n",
        "        # Define preprocessing for numerical features\n",
        "        numerical_features = [col for col in self.feature_columns\n",
        "                            if self.train[col].dtype in ['float64', 'int64']]\n",
        "        numerical_transformer = Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='median')),\n",
        "            ('scaler', StandardScaler())\n",
        "        ])\n",
        "\n",
        "        # Define preprocessing for categorical features\n",
        "        categorical_features = [col for col in self.feature_columns\n",
        "                              if self.train[col].dtype == 'object']\n",
        "        categorical_transformer = Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "        ])\n",
        "\n",
        "        # Combine preprocessing steps\n",
        "        preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', numerical_transformer, numerical_features),\n",
        "                ('cat', categorical_transformer, categorical_features)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Define the pipeline\n",
        "        self.pipeline = Pipeline(steps=[\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('model', CustomXGBRegressor(\n",
        "                n_estimators=100,\n",
        "                learning_rate=0.1,\n",
        "                max_depth=5,\n",
        "                random_state=42,\n",
        "                objective='count:poisson',  # Better for count data\n",
        "                tree_method='hist',         # Faster training\n",
        "                min_child_weight=1,         # Help with many zero values\n",
        "                subsample=0.8,              # Prevent overfitting\n",
        "                colsample_bytree=0.8        # Prevent overfitting\n",
        "            ))\n",
        "        ])\n",
        "\n",
        "    def run_pipeline(self):\n",
        "        try:\n",
        "            # Load and preprocess data\n",
        "            self.load_data()\n",
        "\n",
        "            # Prepare training data\n",
        "            X = self.train[self.feature_columns]\n",
        "            y = self.train['Total']\n",
        "\n",
        "            # Split the training data\n",
        "            X_train, X_val, y_train, y_val = train_test_split(\n",
        "                X, y, test_size=0.2, random_state=42\n",
        "            )\n",
        "\n",
        "            # Create and train the pipeline\n",
        "            self.create_pipeline()\n",
        "            self.pipeline.fit(X_train, y_train)\n",
        "\n",
        "            # Evaluate on validation set\n",
        "            val_predictions = self.pipeline.predict(X_val)\n",
        "            mae = np.mean(np.abs(val_predictions - y_val))\n",
        "            print(f\"Validation MAE: {mae}\")\n",
        "\n",
        "            # Cross-validation for robustness\n",
        "            cv_scores = cross_val_score(\n",
        "                self.pipeline, X, y,\n",
        "                cv=5,\n",
        "                scoring='neg_mean_absolute_error'\n",
        "            )\n",
        "            print(f\"Cross-validation MAE: {-np.mean(cv_scores):.2f} Â± {np.std(cv_scores):.2f}\")\n",
        "\n",
        "            # Predict on test set\n",
        "            test_predictions = self.pipeline.predict(self.test[self.feature_columns])\n",
        "\n",
        "            # Create submission dataframe\n",
        "            submission = pd.DataFrame({\n",
        "                'ID': self.test['ID'],\n",
        "                'Total': test_predictions\n",
        "            })\n",
        "            submission.to_csv('predictions.csv', index=False)\n",
        "            print(\"Predictions saved to 'predictions.csv'\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pipeline = DiseasePredictionPipeline()\n",
        "    pipeline.run_pipeline()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ef185a1",
      "metadata": {
        "id": "5ef185a1"
      },
      "outputs": [],
      "source": [
        "# Second model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDHviuJ0dvft",
        "outputId": "e4cc4c9e-4c33-4e8f-a7fc-59bed301423c"
      },
      "id": "uDHviuJ0dvft",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.0.0)\n",
            "Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0dbed84a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dbed84a",
        "outputId": "6e24b3c9-36fb-43c7-952e-5a2782f85bac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data Analysis Summary:\n",
            "Total samples: 23848\n",
            "Features: 9\n",
            "\n",
            "Missing Values Summary:\n",
            "ID                               0\n",
            "Total                            1\n",
            "Location                         0\n",
            "Category_Health_Facility_UUID    0\n",
            "Disease                          0\n",
            "Month                            0\n",
            "Year                             0\n",
            "Transformed_Latitude             0\n",
            "Transformed_Longitude            0\n",
            "dtype: int64\n",
            "\n",
            "Outliers Summary:\n",
            "{'Total': 4609, 'Month': 0, 'Year': 0, 'Transformed_Latitude': 3564, 'Transformed_Longitude': 2112}\n",
            "\n",
            "Target Variable Analysis:\n",
            "count    23847.000000\n",
            "mean         8.355600\n",
            "std         28.076713\n",
            "min          0.000000\n",
            "25%          0.000000\n",
            "50%          0.000000\n",
            "75%          3.000000\n",
            "max        489.000000\n",
            "Name: Total, dtype: float64\n",
            "\n",
            "Applied log transformation to target variable due to high skewness\n",
            "\n",
            "Removed 120 rows with invalid or extreme values\n",
            "An error occurred in run_pipeline: The estimator EnhancedXGBRegressor should be a regressor.\n",
            "Error in main: The estimator EnhancedXGBRegressor should be a regressor.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.linear_model import HuberRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class EnhancedRegressorBase(BaseEstimator, RegressorMixin):\n",
        "    \"\"\"Base class for enhanced regressors with robust error handling and logging\"\"\"\n",
        "    def __init__(self, model_class, **params):\n",
        "        self.params = params\n",
        "        self.model = model_class(**self.params)\n",
        "        self.feature_importance_ = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        try:\n",
        "            self.model.fit(X, y)\n",
        "            if hasattr(self.model, 'feature_importances_'):\n",
        "                self.feature_importance_ = self.model.feature_importances_\n",
        "            return self\n",
        "        except Exception as e:\n",
        "            print(f\"Error during fitting: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def predict(self, X):\n",
        "        try:\n",
        "            return self.model.predict(X)\n",
        "        except Exception as e:\n",
        "            print(f\"Error during prediction: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {**{\"model_class\": self.model.__class__}, **self.params}\n",
        "\n",
        "    def set_params(self, **params):\n",
        "        for param, value in params.items():\n",
        "            if param == \"model_class\":\n",
        "                continue\n",
        "            self.params[param] = value\n",
        "        self.model.set_params(**self.params)\n",
        "        return self\n",
        "\n",
        "class EnhancedXGBRegressor(EnhancedRegressorBase):\n",
        "    \"\"\"Enhanced XGBoost regressor with error handling\"\"\"\n",
        "    def __init__(self, **params):\n",
        "        super().__init__(XGBRegressor, **params)\n",
        "\n",
        "class EnhancedLGBMRegressor(EnhancedRegressorBase):\n",
        "    \"\"\"Enhanced LightGBM regressor with error handling\"\"\"\n",
        "    def __init__(self, **params):\n",
        "        super().__init__(LGBMRegressor, **params)\n",
        "\n",
        "class EnhancedCatBoostRegressor(EnhancedRegressorBase):\n",
        "    \"\"\"Enhanced CatBoost regressor with error handling\"\"\"\n",
        "    def __init__(self, **params):\n",
        "        super().__init__(CatBoostRegressor, **params)\n",
        "\n",
        "class ImprovedDiseasePredictionPipeline:\n",
        "    \"\"\"Main pipeline class for disease prediction\"\"\"\n",
        "    def __init__(self):\n",
        "        self.train = None\n",
        "        self.test = None\n",
        "        self.pipeline = None\n",
        "        self.feature_columns = None\n",
        "        self.numeric_features = None\n",
        "        self.categorical_features = None\n",
        "        self.feature_importance = None\n",
        "        self.model_metrics = {}\n",
        "        self.target_transformed = False\n",
        "\n",
        "    def load_and_analyze_data(self, train_path=\"Train.csv\", test_path=\"Test.csv\", waste_path=\"waste_management.csv\",\n",
        "                              toilet_path=\"toilet.csv\", water_path=\"water_sources.csv\"):\n",
        "        \"\"\"Load and perform initial statistical analysis of the data\"\"\"\n",
        "        try:\n",
        "            # Load datasets\n",
        "            self.train = pd.read_csv(train_path)\n",
        "            self.test = pd.read_csv(test_path)\n",
        "            self.test = pd.read_csv(waste_path)\n",
        "            self.test = pd.read_csv(toilet_path)\n",
        "            self.test = pd.read_csv(water_path)\n",
        "\n",
        "            # Statistical analysis\n",
        "            self._perform_statistical_analysis()\n",
        "\n",
        "            # Clean target variable\n",
        "            self._clean_target_variable()\n",
        "\n",
        "            # Define and verify features\n",
        "            self._setup_features()\n",
        "\n",
        "            # Handle missing and anomalous values\n",
        "            self._handle_data_quality()\n",
        "\n",
        "            return self.train, self.test\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in load_and_analyze_data: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _perform_statistical_analysis(self):\n",
        "        \"\"\"Perform comprehensive statistical analysis\"\"\"\n",
        "        try:\n",
        "            stats_report = {\n",
        "                'basic_stats': self.train.describe(),\n",
        "                'missing_values': self.train.isnull().sum(),\n",
        "                'skewness': self.train.select_dtypes(include=[np.number]).skew(),\n",
        "                'kurtosis': self.train.select_dtypes(include=[np.number]).kurtosis()\n",
        "            }\n",
        "\n",
        "            # Detect outliers using IQR method\n",
        "            numeric_cols = self.train.select_dtypes(include=[np.number]).columns\n",
        "            outliers_report = {}\n",
        "            for col in numeric_cols:\n",
        "                Q1 = self.train[col].quantile(0.25)\n",
        "                Q3 = self.train[col].quantile(0.75)\n",
        "                IQR = Q3 - Q1\n",
        "                outliers = ((self.train[col] < (Q1 - 1.5 * IQR)) |\n",
        "                           (self.train[col] > (Q3 + 1.5 * IQR))).sum()\n",
        "                outliers_report[col] = outliers\n",
        "\n",
        "            stats_report['outliers'] = outliers_report\n",
        "\n",
        "            # Print summary statistics\n",
        "            print(\"\\nData Analysis Summary:\")\n",
        "            print(f\"Total samples: {len(self.train)}\")\n",
        "            print(f\"Features: {len(self.train.columns)}\")\n",
        "            print(\"\\nMissing Values Summary:\")\n",
        "            print(stats_report['missing_values'])\n",
        "            print(\"\\nOutliers Summary:\")\n",
        "            print(stats_report['outliers'])\n",
        "\n",
        "            return stats_report\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in _perform_statistical_analysis: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _clean_target_variable(self):\n",
        "        \"\"\"Clean and transform target variable\"\"\"\n",
        "        try:\n",
        "            print(\"\\nTarget Variable Analysis:\")\n",
        "            print(self.train['Total'].describe())\n",
        "\n",
        "            # Remove invalid values (only remove extreme outliers)\n",
        "            original_len = len(self.train)\n",
        "            self.train = self.train[\n",
        "                (np.isfinite(self.train['Total'])) &\n",
        "                (self.train['Total'] >= 0) &\n",
        "                (self.train['Total'] <= self.train['Total'].quantile(0.995))  # Less aggressive outlier removal\n",
        "            ]\n",
        "\n",
        "            # Apply log transformation if highly skewed\n",
        "            if stats.skew(self.train['Total']) > 1:\n",
        "                self.train['Total'] = np.log1p(self.train['Total'])\n",
        "                self.target_transformed = True\n",
        "                print(\"\\nApplied log transformation to target variable due to high skewness\")\n",
        "\n",
        "            print(f\"\\nRemoved {original_len - len(self.train)} rows with invalid or extreme values\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in _clean_target_variable: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _setup_features(self):\n",
        "        \"\"\"Setup and verify features\"\"\"\n",
        "        try:\n",
        "            self.feature_columns = [col for col in self.train.columns\n",
        "                                  if col not in ['Total', 'ID']]\n",
        "\n",
        "            # Verify features\n",
        "            missing_cols = [col for col in self.feature_columns\n",
        "                           if col not in self.test.columns]\n",
        "            if missing_cols:\n",
        "                raise ValueError(f\"Missing columns in test dataset: {missing_cols}\")\n",
        "\n",
        "            # Analyze feature types\n",
        "            self.numeric_features = self.train[self.feature_columns].select_dtypes(\n",
        "                include=['float64', 'int64']).columns.tolist()\n",
        "            self.categorical_features = self.train[self.feature_columns].select_dtypes(\n",
        "                include=['object']).columns.tolist()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in _setup_features: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _handle_data_quality(self):\n",
        "        \"\"\"Handle missing values and anomalies\"\"\"\n",
        "        try:\n",
        "            for df in [self.train, self.test]:\n",
        "                # Handle numeric features\n",
        "                for col in self.numeric_features:\n",
        "                    # Replace infinite values\n",
        "                    df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "                    # Handle outliers using winsorization\n",
        "                    if col in df.columns:\n",
        "                        q1 = df[col].quantile(0.01)\n",
        "                        q3 = df[col].quantile(0.99)\n",
        "                        df[col] = df[col].clip(q1, q3)\n",
        "\n",
        "                # Handle categorical features\n",
        "                for col in self.categorical_features:\n",
        "                    if col in df.columns:\n",
        "                        # Convert rare categories to 'Other'\n",
        "                        value_counts = df[col].value_counts()\n",
        "                        rare_categories = value_counts[value_counts < len(df) * 0.01].index\n",
        "                        df[col] = df[col].replace(rare_categories, 'Other')\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in _handle_data_quality: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def create_advanced_pipeline(self):\n",
        "        \"\"\"Create an advanced pipeline with robust preprocessing and stacking\"\"\"\n",
        "        try:\n",
        "            # Numeric preprocessing\n",
        "            numeric_transformer = Pipeline(steps=[\n",
        "                ('imputer', KNNImputer(n_neighbors=5)),\n",
        "                ('scaler', RobustScaler()),\n",
        "                ('power', PowerTransformer(standardize=True))\n",
        "            ])\n",
        "\n",
        "            # Categorical preprocessing\n",
        "            categorical_transformer = Pipeline(steps=[\n",
        "                ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "                ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "            ])\n",
        "\n",
        "            # Create preprocessor\n",
        "            preprocessor = ColumnTransformer(\n",
        "                transformers=[\n",
        "                    ('num', numeric_transformer, self.numeric_features),\n",
        "                    ('cat', categorical_transformer, self.categorical_features)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            # Define base models with correct initialization\n",
        "            base_models = [\n",
        "                ('xgb', EnhancedXGBRegressor(\n",
        "                    n_estimators=200,\n",
        "                    learning_rate=0.05,\n",
        "                    max_depth=6,\n",
        "                    subsample=0.8,\n",
        "                    colsample_bytree=0.8,\n",
        "                    random_state=42\n",
        "                )),\n",
        "                ('lgbm', EnhancedLGBMRegressor(\n",
        "                    n_estimators=200,\n",
        "                    learning_rate=0.05,\n",
        "                    num_leaves=31,\n",
        "                    subsample=0.8,\n",
        "                    colsample_bytree=0.8,\n",
        "                    random_state=42\n",
        "                )),\n",
        "                ('catboost', EnhancedCatBoostRegressor(\n",
        "                    iterations=200,\n",
        "                    learning_rate=0.05,\n",
        "                    depth=6,\n",
        "                    subsample=0.8,\n",
        "                    random_state=42,\n",
        "                    verbose=False\n",
        "                ))\n",
        "            ]\n",
        "\n",
        "            # Create stacking model\n",
        "            final_estimator = HuberRegressor()\n",
        "            stacking = StackingRegressor(\n",
        "                estimators=base_models,\n",
        "                final_estimator=final_estimator,\n",
        "                cv=5,\n",
        "                n_jobs=-1\n",
        "            )\n",
        "\n",
        "            # Create final pipeline\n",
        "            self.pipeline = Pipeline(steps=[\n",
        "                ('preprocessor', preprocessor),\n",
        "                ('stacking', stacking)\n",
        "            ])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in create_advanced_pipeline: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def evaluate_model(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"Evaluate model performance with detailed metrics\"\"\"\n",
        "        try:\n",
        "            if X_val is not None and y_val is not None:\n",
        "                # Validation set metrics\n",
        "                val_pred = self.pipeline.predict(X_val)\n",
        "                self.model_metrics['validation'] = {\n",
        "                    'mae': mean_absolute_error(y_val, val_pred),\n",
        "                    'rmse': np.sqrt(mean_squared_error(y_val, val_pred)),\n",
        "                    'r2': r2_score(y_val, val_pred)\n",
        "                }\n",
        "\n",
        "            # Cross-validation metrics\n",
        "            cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "            cv_scores = cross_val_score(\n",
        "                self.pipeline, X, y,\n",
        "                cv=cv,\n",
        "                scoring='neg_mean_absolute_error',\n",
        "                n_jobs=-1\n",
        "            )\n",
        "\n",
        "            self.model_metrics['cross_validation'] = {\n",
        "                'mae_mean': -np.mean(cv_scores),\n",
        "                'mae_std': np.std(cv_scores)\n",
        "            }\n",
        "\n",
        "            # Print evaluation results\n",
        "            print(\"\\nModel Evaluation Results:\")\n",
        "            if 'validation' in self.model_metrics:\n",
        "                print(\"\\nValidation Metrics:\")\n",
        "                print(f\"MAE: {self.model_metrics['validation']['mae']:.4f}\")\n",
        "                print(f\"RMSE: {self.model_metrics['validation']['rmse']:.4f}\")\n",
        "                print(f\"RÂ²: {self.model_metrics['validation']['r2']:.4f}\")\n",
        "\n",
        "            print(\"\\nCross-validation Metrics:\")\n",
        "            print(f\"MAE: {self.model_metrics['cross_validation']['mae_mean']:.4f} Â± \"\n",
        "                  f\"{self.model_metrics['cross_validation']['mae_std']:.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in evaluate_model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def run_pipeline(self, train_path=\"Train.csv\", test_path=\"Test.csv\"):\n",
        "        \"\"\"Run the complete pipeline\"\"\"\n",
        "        try:\n",
        "            # Load and analyze data\n",
        "            self.load_and_analyze_data(train_path, test_path)\n",
        "\n",
        "            # Prepare data\n",
        "            X = self.train[self.feature_columns]\n",
        "            y = self.train['Total']\n",
        "\n",
        "            # Split data\n",
        "            X_train, X_val, y_train, y_val = train_test_split(\n",
        "                X, y, test_size=0.2, random_state=42\n",
        "            )\n",
        "\n",
        "            # Create and train pipeline\n",
        "            self.create_advanced_pipeline()\n",
        "            self.pipeline.fit(X_train, y_train)\n",
        "\n",
        "            # Evaluate model\n",
        "            self.evaluate_model(X_train, y_train, X_val, y_val)\n",
        "\n",
        "            # Generate predictions\n",
        "            test_predictions = self.pipeline.predict(self.test[self.feature_columns])\n",
        "\n",
        "            # If target was log-transformed, reverse transform predictions\n",
        "            if self.target_transformed:\n",
        "                test_predictions = np.expm1(test_predictions)\n",
        "\n",
        "            # Create submission\n",
        "            submission = pd.DataFrame({\n",
        "                'ID': self.test['ID'],\n",
        "                'Total': test_predictions\n",
        "            })\n",
        "            submission.to_csv('predictions.csv', index=False)\n",
        "            print(\"\\nPredictions saved to 'predictions.csv'\")\n",
        "\n",
        "            return submission\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred in run_pipeline: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        pipeline = ImprovedDiseasePredictionPipeline()\n",
        "        pipeline.run_pipeline()\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# updating it model 3"
      ],
      "metadata": {
        "id": "BN-k5VY8jikM"
      },
      "id": "BN-k5VY8jikM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.linear_model import HuberRegressor\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "import warnings\n",
        "import os\n",
        "from datetime import datetime\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class EnhancedRegressorBase(BaseEstimator, RegressorMixin):\n",
        "    \"\"\"Base class for enhanced regressors with robust error handling and logging\"\"\"\n",
        "    def __init__(self, model_class, **params):\n",
        "        self.params = params\n",
        "        self.model = model_class(**self.params)\n",
        "        self.feature_importance_ = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        try:\n",
        "            self.model.fit(X, y)\n",
        "            if hasattr(self.model, 'feature_importances_'):\n",
        "                self.feature_importance_ = self.model.feature_importances_\n",
        "            return self\n",
        "        except Exception as e:\n",
        "            print(f\"Error during fitting: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def predict(self, X):\n",
        "        try:\n",
        "            return self.model.predict(X)\n",
        "        except Exception as e:\n",
        "            print(f\"Error during prediction: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {**{\"model_class\": self.model.__class__}, **self.params}\n",
        "\n",
        "    def set_params(self, **params):\n",
        "        for param, value in params.items():\n",
        "            if param == \"model_class\":\n",
        "                continue\n",
        "            self.params[param] = value\n",
        "        self.model = self.model.__class__(**self.params)\n",
        "        return self\n",
        "\n",
        "class EnhancedXGBRegressor(EnhancedRegressorBase):\n",
        "    def __init__(self, **params):\n",
        "        super().__init__(XGBRegressor, **params)\n",
        "\n",
        "class EnhancedLGBMRegressor(EnhancedRegressorBase):\n",
        "    def __init__(self, **params):\n",
        "        super().__init__(LGBMRegressor, **params)\n",
        "\n",
        "class EnhancedCatBoostRegressor(EnhancedRegressorBase):\n",
        "    def __init__(self, **params):\n",
        "        super().__init__(CatBoostRegressor, **params)\n",
        "\n",
        "class ImprovedDiseasePredictionPipeline:\n",
        "    def __init__(self, base_dir='.'):\n",
        "        self.train = None\n",
        "        self.test = None\n",
        "        self.pipeline = None\n",
        "        self.feature_columns = None\n",
        "        self.numeric_features = None\n",
        "        self.categorical_features = None\n",
        "        self.feature_importance = None\n",
        "        self.model_metrics = {}\n",
        "        self.target_transformed = False\n",
        "        self.base_dir = base_dir\n",
        "\n",
        "    def load_and_analyze_data(self, train_path=\"Train.csv\", test_path=\"Test.csv\",\n",
        "                            waste_path=\"waste_management.csv\", toilet_path=\"toilets.csv\",\n",
        "                            water_path=\"water_sources.csv\"):\n",
        "        try:\n",
        "            # Convert relative paths to absolute paths\n",
        "            train_path = os.path.join(self.base_dir, train_path)\n",
        "            test_path = os.path.join(self.base_dir, test_path)\n",
        "            waste_path = os.path.join(self.base_dir, waste_path)\n",
        "            toilet_path = os.path.join(self.base_dir, toilet_path)\n",
        "            water_path = os.path.join(self.base_dir, water_path)\n",
        "\n",
        "            self.train = pd.read_csv(train_path)\n",
        "            self.test = pd.read_csv(test_path)\n",
        "            waste_data = pd.read_csv(waste_path)\n",
        "            toilet_data = pd.read_csv(toilet_path)\n",
        "            water_data = pd.read_csv(water_path)\n",
        "\n",
        "            # Proper merge with error handling\n",
        "            for df in [self.train, self.test]:\n",
        "                df = df.merge(waste_data, on='ID', how='left')\n",
        "                df = df.merge(toilet_data, on='ID', how='left')\n",
        "                df = df.merge(water_data, on='ID', how='left')\n",
        "\n",
        "            self._perform_statistical_analysis()\n",
        "            self._clean_target_variable()\n",
        "            self._setup_features()\n",
        "            self._handle_data_quality()\n",
        "\n",
        "            return self.train, self.test\n",
        "        except Exception as e:\n",
        "            print(f\"Error in load_and_analyze_data: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def evaluate_and_predict(self, output_path=None):\n",
        "        try:\n",
        "            # Generate default output path in the same directory\n",
        "            if output_path is None:\n",
        "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "                output_path = os.path.join(self.base_dir, f\"predictions_{timestamp}.csv\")\n",
        "\n",
        "            X = self.train[self.feature_columns]\n",
        "            y = self.train['Total']\n",
        "            X_test = self.test[self.feature_columns]\n",
        "\n",
        "            # Fit the pipeline and generate predictions\n",
        "            print(\"\\nFitting model...\")\n",
        "            self.pipeline.fit(X, y)\n",
        "\n",
        "            print(\"Generating predictions...\")\n",
        "            predictions = self.pipeline.predict(X_test)\n",
        "\n",
        "            # Reverse log transformation if applied\n",
        "            if self.target_transformed:\n",
        "                predictions = np.expm1(predictions)\n",
        "\n",
        "            # Prepare submission DataFrame\n",
        "            submission_df = pd.DataFrame({\n",
        "                'ID': self.test['ID'].astype(str) + \"_2023_Diarrhea\",\n",
        "                'Target': predictions\n",
        "            })\n",
        "\n",
        "            # Save predictions\n",
        "            submission_df.to_csv(output_path, index=False)\n",
        "            print(f\"\\nPredictions saved to: {output_path}\")\n",
        "            print(\"\\nFirst 10 predictions:\")\n",
        "            print(submission_df.head(10))\n",
        "\n",
        "            # Evaluate model\n",
        "            y_pred = self.pipeline.predict(X)\n",
        "            if self.target_transformed:\n",
        "                y_pred = np.expm1(y_pred)\n",
        "                y = np.expm1(y)\n",
        "\n",
        "            # Calculate and store metrics\n",
        "            self.model_metrics.update({\n",
        "                'MAE': mean_absolute_error(y, y_pred),\n",
        "                'MSE': mean_squared_error(y, y_pred),\n",
        "                'RMSE': np.sqrt(mean_squared_error(y, y_pred)),\n",
        "                'R2': r2_score(y, y_pred)\n",
        "            })\n",
        "\n",
        "            # Print evaluation metrics\n",
        "            print(\"\\nModel Evaluation Metrics:\")\n",
        "            for metric, value in self.model_metrics.items():\n",
        "                print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "            return submission_df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in evaluate_and_predict: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    pipeline = ImprovedDiseasePredictionPipeline()\n",
        "    pipeline.load_and_analyze_data()\n",
        "    pipeline.create_advanced_pipeline()\n",
        "    predictions = pipeline.evaluate_and_predict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "96C5dAzqjlb0",
        "outputId": "c209ef66-035f-43db-e524-80303697537d"
      },
      "id": "96C5dAzqjlb0",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in load_and_analyze_data: [Errno 2] No such file or directory: './toilet.csv'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './toilet.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-e9236c47097f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImprovedDiseasePredictionPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_and_analyze_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_advanced_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_and_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-e9236c47097f>\u001b[0m in \u001b[0;36mload_and_analyze_data\u001b[0;34m(self, train_path, test_path, waste_path, toilet_path, water_path)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mwaste_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwaste_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mtoilet_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoilet_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0mwater_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwater_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './toilet.csv'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}