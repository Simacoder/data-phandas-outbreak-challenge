{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Simacoder/data-phandas-outbreak-challenge/blob/main/Model7%20updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c70b70c0",
      "metadata": {
        "id": "c70b70c0"
      },
      "source": [
        "# Model7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b95ce335",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b95ce335",
        "outputId": "39e18c13-c5b2-4017-f628-43c8c8087ef4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in training dataset: Index(['ID', 'Total', 'Location', 'Category_Health_Facility_UUID', 'Disease',\n",
            "       'Month', 'Year', 'Transformed_Latitude', 'Transformed_Longitude'],\n",
            "      dtype='object')\n",
            "Columns in testing dataset: Index(['Location', 'Disease', 'Month', 'Category_Health_Facility_UUID', 'Year',\n",
            "       'Transformed_Latitude', 'Transformed_Longitude', 'ID'],\n",
            "      dtype='object')\n",
            "\n",
            "Initial target variable statistics:\n",
            "count    23847.000000\n",
            "mean         8.355600\n",
            "std         28.076713\n",
            "min          0.000000\n",
            "25%          0.000000\n",
            "50%          0.000000\n",
            "75%          3.000000\n",
            "max        489.000000\n",
            "Name: Total, dtype: float64\n",
            "\n",
            "Target variable statistics after cleaning:\n",
            "count    23847.000000\n",
            "mean         8.355600\n",
            "std         28.076713\n",
            "min          0.000000\n",
            "25%          0.000000\n",
            "50%          0.000000\n",
            "75%          3.000000\n",
            "max        489.000000\n",
            "Name: Total, dtype: float64\n",
            "\n",
            "Removed 0 rows with invalid Total values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-796a3c6b0995>:81: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(df[col].median(), inplace=True)\n",
            "<ipython-input-2-796a3c6b0995>:87: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(df[col].mode()[0], inplace=True)\n",
            "<ipython-input-2-796a3c6b0995>:81: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(df[col].median(), inplace=True)\n",
            "<ipython-input-2-796a3c6b0995>:87: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(df[col].mode()[0], inplace=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation MAE: 9.90429589219813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation MAE: 11.14 Â± 2.23\n",
            "Predictions saved to 'predictions.csv'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Custom wrapper for XGBRegressor to ensure compatibility\n",
        "class CustomXGBRegressor(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, **params):\n",
        "        self.model = XGBRegressor(**params)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.model.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return self.model.get_params(deep)\n",
        "\n",
        "    def set_params(self, **params):\n",
        "        self.model.set_params(**params)\n",
        "        return self\n",
        "\n",
        "class DiseasePredictionPipeline:\n",
        "    def __init__(self):\n",
        "        self.train = None\n",
        "        self.test = None\n",
        "        self.pipeline = None\n",
        "        self.feature_columns = None\n",
        "\n",
        "    def load_data(self):\n",
        "        # Load datasets\n",
        "        self.train = pd.read_csv(\"Train.csv\")\n",
        "        self.test = pd.read_csv(\"Test.csv\")\n",
        "\n",
        "        print(\"Columns in training dataset:\", self.train.columns)\n",
        "        print(\"Columns in testing dataset:\", self.test.columns)\n",
        "\n",
        "        # Clean target variable\n",
        "        print(\"\\nInitial target variable statistics:\")\n",
        "        print(self.train['Total'].describe())\n",
        "\n",
        "        # Remove rows where Total is NaN, infinite, or extremely large\n",
        "        self.train = self.train[\n",
        "            np.isfinite(self.train['Total'])\n",
        "        ]\n",
        "\n",
        "        print(\"\\nTarget variable statistics after cleaning:\")\n",
        "        print(self.train['Total'].describe())\n",
        "        print(f\"\\nRemoved {len(self.train) - len(self.train)} rows with invalid Total values\")\n",
        "\n",
        "        # Define feature columns (excluding 'Total' and 'ID')\n",
        "        self.feature_columns = [col for col in self.train.columns\n",
        "                              if col not in ['Total', 'ID']]\n",
        "\n",
        "        # Verify all feature columns exist in test dataset\n",
        "        missing_cols = [col for col in self.feature_columns\n",
        "                       if col not in self.test.columns]\n",
        "        if missing_cols:\n",
        "            raise ValueError(f\"Missing columns in test dataset: {missing_cols}\")\n",
        "\n",
        "        # Handle missing values separately for train and test\n",
        "        self._handle_missing_values(self.train)\n",
        "        self._handle_missing_values(self.test)\n",
        "\n",
        "    def _handle_missing_values(self, df):\n",
        "        \"\"\"Handle missing values for a given dataframe\"\"\"\n",
        "        # Numerical columns\n",
        "        numerical_cols = df[self.feature_columns].select_dtypes(\n",
        "            include=['float64', 'int64']).columns\n",
        "        for col in numerical_cols:\n",
        "            # Replace infinite values with NaN first\n",
        "            df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
        "            # Then fill NaN with median\n",
        "            df[col].fillna(df[col].median(), inplace=True)\n",
        "\n",
        "        # Categorical columns\n",
        "        categorical_cols = df[self.feature_columns].select_dtypes(\n",
        "            include=['object']).columns\n",
        "        for col in categorical_cols:\n",
        "            df[col].fillna(df[col].mode()[0], inplace=True)\n",
        "\n",
        "    def create_pipeline(self):\n",
        "        # Define preprocessing for numerical features\n",
        "        numerical_features = [col for col in self.feature_columns\n",
        "                            if self.train[col].dtype in ['float64', 'int64']]\n",
        "        numerical_transformer = Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='median')),\n",
        "            ('scaler', StandardScaler())\n",
        "        ])\n",
        "\n",
        "        # Define preprocessing for categorical features\n",
        "        categorical_features = [col for col in self.feature_columns\n",
        "                              if self.train[col].dtype == 'object']\n",
        "        categorical_transformer = Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "        ])\n",
        "\n",
        "        # Combine preprocessing steps\n",
        "        preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', numerical_transformer, numerical_features),\n",
        "                ('cat', categorical_transformer, categorical_features)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Define the pipeline\n",
        "        self.pipeline = Pipeline(steps=[\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('model', CustomXGBRegressor(\n",
        "                n_estimators=100,\n",
        "                learning_rate=0.1,\n",
        "                max_depth=5,\n",
        "                random_state=42,\n",
        "                objective='count:poisson',  # Better for count data\n",
        "                tree_method='hist',         # Faster training\n",
        "                min_child_weight=1,         # Help with many zero values\n",
        "                subsample=0.8,              # Prevent overfitting\n",
        "                colsample_bytree=0.8        # Prevent overfitting\n",
        "            ))\n",
        "        ])\n",
        "\n",
        "    def run_pipeline(self):\n",
        "        try:\n",
        "            # Load and preprocess data\n",
        "            self.load_data()\n",
        "\n",
        "            # Prepare training data\n",
        "            X = self.train[self.feature_columns]\n",
        "            y = self.train['Total']\n",
        "\n",
        "            # Split the training data\n",
        "            X_train, X_val, y_train, y_val = train_test_split(\n",
        "                X, y, test_size=0.2, random_state=42\n",
        "            )\n",
        "\n",
        "            # Create and train the pipeline\n",
        "            self.create_pipeline()\n",
        "            self.pipeline.fit(X_train, y_train)\n",
        "\n",
        "            # Evaluate on validation set\n",
        "            val_predictions = self.pipeline.predict(X_val)\n",
        "            mae = np.mean(np.abs(val_predictions - y_val))\n",
        "            print(f\"Validation MAE: {mae}\")\n",
        "\n",
        "            # Cross-validation for robustness\n",
        "            cv_scores = cross_val_score(\n",
        "                self.pipeline, X, y,\n",
        "                cv=5,\n",
        "                scoring='neg_mean_absolute_error'\n",
        "            )\n",
        "            print(f\"Cross-validation MAE: {-np.mean(cv_scores):.2f} Â± {np.std(cv_scores):.2f}\")\n",
        "\n",
        "            # Predict on test set\n",
        "            test_predictions = self.pipeline.predict(self.test[self.feature_columns])\n",
        "\n",
        "            # Create submission dataframe\n",
        "            submission = pd.DataFrame({\n",
        "                'ID': self.test['ID'],\n",
        "                'Total': test_predictions\n",
        "            })\n",
        "            submission.to_csv('predictions.csv', index=False)\n",
        "            print(\"Predictions saved to 'predictions.csv'\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pipeline = DiseasePredictionPipeline()\n",
        "    pipeline.run_pipeline()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ef185a1",
      "metadata": {
        "id": "5ef185a1"
      },
      "outputs": [],
      "source": [
        "# Second model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDHviuJ0dvft",
        "outputId": "e4cc4c9e-4c33-4e8f-a7fc-59bed301423c"
      },
      "id": "uDHviuJ0dvft",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.0.0)\n",
            "Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0dbed84a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "0dbed84a",
        "outputId": "58b2453b-d349-4eaa-b6e9-cb9448cfb20f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'catboost'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1c01418ef976>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLGBMRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCatBoostRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStackingRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHuberRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'catboost'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.linear_model import HuberRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class EnhancedRegressorBase(BaseEstimator, RegressorMixin):\n",
        "    \"\"\"Base class for enhanced regressors with robust error handling and logging\"\"\"\n",
        "    def __init__(self, model_class, **params):\n",
        "        self.params = params\n",
        "        self.model = model_class(**self.params)\n",
        "        self.feature_importance_ = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        try:\n",
        "            self.model.fit(X, y)\n",
        "            if hasattr(self.model, 'feature_importances_'):\n",
        "                self.feature_importance_ = self.model.feature_importances_\n",
        "            return self\n",
        "        except Exception as e:\n",
        "            print(f\"Error during fitting: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def predict(self, X):\n",
        "        try:\n",
        "            return self.model.predict(X)\n",
        "        except Exception as e:\n",
        "            print(f\"Error during prediction: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {**{\"model_class\": self.model.__class__}, **self.params}\n",
        "\n",
        "    def set_params(self, **params):\n",
        "        for param, value in params.items():\n",
        "            if param == \"model_class\":\n",
        "                continue\n",
        "            self.params[param] = value\n",
        "        self.model.set_params(**self.params)\n",
        "        return self\n",
        "\n",
        "class EnhancedXGBRegressor(EnhancedRegressorBase):\n",
        "    \"\"\"Enhanced XGBoost regressor with error handling\"\"\"\n",
        "    def __init__(self, **params):\n",
        "        super().__init__(XGBRegressor, **params)\n",
        "\n",
        "class EnhancedLGBMRegressor(EnhancedRegressorBase):\n",
        "    \"\"\"Enhanced LightGBM regressor with error handling\"\"\"\n",
        "    def __init__(self, **params):\n",
        "        super().__init__(LGBMRegressor, **params)\n",
        "\n",
        "class EnhancedCatBoostRegressor(EnhancedRegressorBase):\n",
        "    \"\"\"Enhanced CatBoost regressor with error handling\"\"\"\n",
        "    def __init__(self, **params):\n",
        "        super().__init__(CatBoostRegressor, **params)\n",
        "\n",
        "class ImprovedDiseasePredictionPipeline:\n",
        "    \"\"\"Main pipeline class for disease prediction\"\"\"\n",
        "    def __init__(self):\n",
        "        self.train = None\n",
        "        self.test = None\n",
        "        self.pipeline = None\n",
        "        self.feature_columns = None\n",
        "        self.numeric_features = None\n",
        "        self.categorical_features = None\n",
        "        self.feature_importance = None\n",
        "        self.model_metrics = {}\n",
        "        self.target_transformed = False\n",
        "\n",
        "    def load_and_analyze_data(self, waste_path=\"waste_management.csv\",\n",
        "                              toilet_path=\"toilet.csv\", water_path=\"water_sources.csv\"):\n",
        "        \"\"\"Load and perform initial statistical analysis of the data\"\"\"\n",
        "        try:\n",
        "            # Load datasets\n",
        "\n",
        "            self.test = pd.read_csv(waste_path)\n",
        "            self.test = pd.read_csv(toilet_path)\n",
        "            self.test = pd.read_csv(water_path)\n",
        "\n",
        "            # Statistical analysis\n",
        "            self._perform_statistical_analysis()\n",
        "\n",
        "            # Clean target variable\n",
        "            self._clean_target_variable()\n",
        "\n",
        "            # Define and verify features\n",
        "            self._setup_features()\n",
        "\n",
        "            # Handle missing and anomalous values\n",
        "            self._handle_data_quality()\n",
        "\n",
        "            return self.train, self.test\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in load_and_analyze_data: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _perform_statistical_analysis(self):\n",
        "        \"\"\"Perform comprehensive statistical analysis\"\"\"\n",
        "        try:\n",
        "            stats_report = {\n",
        "                'basic_stats': self.train.describe(),\n",
        "                'missing_values': self.train.isnull().sum(),\n",
        "                'skewness': self.train.select_dtypes(include=[np.number]).skew(),\n",
        "                'kurtosis': self.train.select_dtypes(include=[np.number]).kurtosis()\n",
        "            }\n",
        "\n",
        "            # Detect outliers using IQR method\n",
        "            numeric_cols = self.train.select_dtypes(include=[np.number]).columns\n",
        "            outliers_report = {}\n",
        "            for col in numeric_cols:\n",
        "                Q1 = self.train[col].quantile(0.25)\n",
        "                Q3 = self.train[col].quantile(0.75)\n",
        "                IQR = Q3 - Q1\n",
        "                outliers = ((self.train[col] < (Q1 - 1.5 * IQR)) |\n",
        "                           (self.train[col] > (Q3 + 1.5 * IQR))).sum()\n",
        "                outliers_report[col] = outliers\n",
        "\n",
        "            stats_report['outliers'] = outliers_report\n",
        "\n",
        "            # Print summary statistics\n",
        "            print(\"\\nData Analysis Summary:\")\n",
        "            print(f\"Total samples: {len(self.train)}\")\n",
        "            print(f\"Features: {len(self.train.columns)}\")\n",
        "            print(\"\\nMissing Values Summary:\")\n",
        "            print(stats_report['missing_values'])\n",
        "            print(\"\\nOutliers Summary:\")\n",
        "            print(stats_report['outliers'])\n",
        "\n",
        "            return stats_report\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in _perform_statistical_analysis: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _clean_target_variable(self):\n",
        "        \"\"\"Clean and transform target variable\"\"\"\n",
        "        try:\n",
        "            print(\"\\nTarget Variable Analysis:\")\n",
        "            print(self.train['Total'].describe())\n",
        "\n",
        "            # Remove invalid values (only remove extreme outliers)\n",
        "            original_len = len(self.train)\n",
        "            self.train = self.train[\n",
        "                (np.isfinite(self.train['Total'])) &\n",
        "                (self.train['Total'] >= 0) &\n",
        "                (self.train['Total'] <= self.train['Total'].quantile(0.995))  # Less aggressive outlier removal\n",
        "            ]\n",
        "\n",
        "            # Apply log transformation if highly skewed\n",
        "            if stats.skew(self.train['Total']) > 1:\n",
        "                self.train['Total'] = np.log1p(self.train['Total'])\n",
        "                self.target_transformed = True\n",
        "                print(\"\\nApplied log transformation to target variable due to high skewness\")\n",
        "\n",
        "            print(f\"\\nRemoved {original_len - len(self.train)} rows with invalid or extreme values\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in _clean_target_variable: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _setup_features(self):\n",
        "        \"\"\"Setup and verify features\"\"\"\n",
        "        try:\n",
        "            self.feature_columns = [col for col in self.train.columns\n",
        "                                  if col not in ['Total', 'ID']]\n",
        "\n",
        "            # Verify features\n",
        "            missing_cols = [col for col in self.feature_columns\n",
        "                           if col not in self.test.columns]\n",
        "            if missing_cols:\n",
        "                raise ValueError(f\"Missing columns in test dataset: {missing_cols}\")\n",
        "\n",
        "            # Analyze feature types\n",
        "            self.numeric_features = self.train[self.feature_columns].select_dtypes(\n",
        "                include=['float64', 'int64']).columns.tolist()\n",
        "            self.categorical_features = self.train[self.feature_columns].select_dtypes(\n",
        "                include=['object']).columns.tolist()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in _setup_features: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _handle_data_quality(self):\n",
        "        \"\"\"Handle missing values and anomalies\"\"\"\n",
        "        try:\n",
        "            for df in [self.train, self.test]:\n",
        "                # Handle numeric features\n",
        "                for col in self.numeric_features:\n",
        "                    # Replace infinite values\n",
        "                    df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "                    # Handle outliers using winsorization\n",
        "                    if col in df.columns:\n",
        "                        q1 = df[col].quantile(0.01)\n",
        "                        q3 = df[col].quantile(0.99)\n",
        "                        df[col] = df[col].clip(q1, q3)\n",
        "\n",
        "                # Handle categorical features\n",
        "                for col in self.categorical_features:\n",
        "                    if col in df.columns:\n",
        "                        # Convert rare categories to 'Other'\n",
        "                        value_counts = df[col].value_counts()\n",
        "                        rare_categories = value_counts[value_counts < len(df) * 0.01].index\n",
        "                        df[col] = df[col].replace(rare_categories, 'Other')\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in _handle_data_quality: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def create_advanced_pipeline(self):\n",
        "        \"\"\"Create an advanced pipeline with robust preprocessing and stacking\"\"\"\n",
        "        try:\n",
        "            # Numeric preprocessing\n",
        "            numeric_transformer = Pipeline(steps=[\n",
        "                ('imputer', KNNImputer(n_neighbors=5)),\n",
        "                ('scaler', RobustScaler()),\n",
        "                ('power', PowerTransformer(standardize=True))\n",
        "            ])\n",
        "\n",
        "            # Categorical preprocessing\n",
        "            categorical_transformer = Pipeline(steps=[\n",
        "                ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "                ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "            ])\n",
        "\n",
        "            # Create preprocessor\n",
        "            preprocessor = ColumnTransformer(\n",
        "                transformers=[\n",
        "                    ('num', numeric_transformer, self.numeric_features),\n",
        "                    ('cat', categorical_transformer, self.categorical_features)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            # Define base models with correct initialization\n",
        "            base_models = [\n",
        "                ('xgb', EnhancedXGBRegressor(\n",
        "                    n_estimators=200,\n",
        "                    learning_rate=0.05,\n",
        "                    max_depth=6,\n",
        "                    subsample=0.8,\n",
        "                    colsample_bytree=0.8,\n",
        "                    random_state=42\n",
        "                )),\n",
        "                ('lgbm', EnhancedLGBMRegressor(\n",
        "                    n_estimators=200,\n",
        "                    learning_rate=0.05,\n",
        "                    num_leaves=31,\n",
        "                    subsample=0.8,\n",
        "                    colsample_bytree=0.8,\n",
        "                    random_state=42\n",
        "                )),\n",
        "                ('catboost', EnhancedCatBoostRegressor(\n",
        "                    iterations=200,\n",
        "                    learning_rate=0.05,\n",
        "                    depth=6,\n",
        "                    subsample=0.8,\n",
        "                    random_state=42,\n",
        "                    verbose=False\n",
        "                ))\n",
        "            ]\n",
        "\n",
        "            # Create stacking model\n",
        "            final_estimator = HuberRegressor()\n",
        "            stacking = StackingRegressor(\n",
        "                estimators=base_models,\n",
        "                final_estimator=final_estimator,\n",
        "                cv=5,\n",
        "                n_jobs=-1\n",
        "            )\n",
        "\n",
        "            # Create final pipeline\n",
        "            self.pipeline = Pipeline(steps=[\n",
        "                ('preprocessor', preprocessor),\n",
        "                ('stacking', stacking)\n",
        "            ])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in create_advanced_pipeline: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def evaluate_model(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"Evaluate model performance with detailed metrics\"\"\"\n",
        "        try:\n",
        "            if X_val is not None and y_val is not None:\n",
        "                # Validation set metrics\n",
        "                val_pred = self.pipeline.predict(X_val)\n",
        "                self.model_metrics['validation'] = {\n",
        "                    'mae': mean_absolute_error(y_val, val_pred),\n",
        "                    'rmse': np.sqrt(mean_squared_error(y_val, val_pred)),\n",
        "                    'r2': r2_score(y_val, val_pred)\n",
        "                }\n",
        "\n",
        "            # Cross-validation metrics\n",
        "            cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "            cv_scores = cross_val_score(\n",
        "                self.pipeline, X, y,\n",
        "                cv=cv,\n",
        "                scoring='neg_mean_absolute_error',\n",
        "                n_jobs=-1\n",
        "            )\n",
        "\n",
        "            self.model_metrics['cross_validation'] = {\n",
        "                'mae_mean': -np.mean(cv_scores),\n",
        "                'mae_std': np.std(cv_scores)\n",
        "            }\n",
        "\n",
        "            # Print evaluation results\n",
        "            print(\"\\nModel Evaluation Results:\")\n",
        "            if 'validation' in self.model_metrics:\n",
        "                print(\"\\nValidation Metrics:\")\n",
        "                print(f\"MAE: {self.model_metrics['validation']['mae']:.4f}\")\n",
        "                print(f\"RMSE: {self.model_metrics['validation']['rmse']:.4f}\")\n",
        "                print(f\"RÂ²: {self.model_metrics['validation']['r2']:.4f}\")\n",
        "\n",
        "            print(\"\\nCross-validation Metrics:\")\n",
        "            print(f\"MAE: {self.model_metrics['cross_validation']['mae_mean']:.4f} Â± \"\n",
        "                  f\"{self.model_metrics['cross_validation']['mae_std']:.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in evaluate_model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def run_pipeline(self, train_path=\"Train.csv\", test_path=\"Test.csv\"):\n",
        "        \"\"\"Run the complete pipeline\"\"\"\n",
        "        try:\n",
        "            # Load and analyze data\n",
        "            self.load_and_analyze_data(train_path, test_path)\n",
        "\n",
        "            # Prepare data\n",
        "            X = self.train[self.feature_columns]\n",
        "            y = self.train['Total']\n",
        "\n",
        "            # Split data\n",
        "            X_train, X_val, y_train, y_val = train_test_split(\n",
        "                X, y, test_size=0.2, random_state=42\n",
        "            )\n",
        "\n",
        "            # Create and train pipeline\n",
        "            self.create_advanced_pipeline()\n",
        "            self.pipeline.fit(X_train, y_train)\n",
        "\n",
        "            # Evaluate model\n",
        "            self.evaluate_model(X_train, y_train, X_val, y_val)\n",
        "\n",
        "            # Generate predictions\n",
        "            test_predictions = self.pipeline.predict(self.test[self.feature_columns])\n",
        "\n",
        "            # If target was log-transformed, reverse transform predictions\n",
        "            if self.target_transformed:\n",
        "                test_predictions = np.expm1(test_predictions)\n",
        "\n",
        "            # Create submission\n",
        "            submission = pd.DataFrame({\n",
        "                'ID': self.test['ID'],\n",
        "                'Total': test_predictions\n",
        "            })\n",
        "            submission.to_csv('predictions.csv', index=False)\n",
        "            print(\"\\nPredictions saved to 'predictions.csv'\")\n",
        "\n",
        "            return submission\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred in run_pipeline: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        pipeline = ImprovedDiseasePredictionPipeline()\n",
        "        pipeline.run_pipeline()\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# updating it model 3"
      ],
      "metadata": {
        "id": "BN-k5VY8jikM"
      },
      "id": "BN-k5VY8jikM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost==1.7.5\n",
        "!pip install scikit-learn==1.2.2"
      ],
      "metadata": {
        "id": "hyDwe_yyPmQe",
        "outputId": "81578e41-1836-4588-b795-81e5c0c28f5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 733
        }
      },
      "id": "hyDwe_yyPmQe",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xgboost==1.7.5\n",
            "  Downloading xgboost-1.7.5-py3-none-manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost==1.7.5) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost==1.7.5) (1.13.1)\n",
            "Downloading xgboost-1.7.5-py3-none-manylinux2014_x86_64.whl (200.3 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m200.3/200.3 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xgboost\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 2.1.3\n",
            "    Uninstalling xgboost-2.1.3:\n",
            "      Successfully uninstalled xgboost-2.1.3\n",
            "Successfully installed xgboost-1.7.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "xgboost"
                ]
              },
              "id": "f6eb8d718df64f74961c8c39b7c40d65"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==1.2.2\n",
            "  Downloading scikit_learn-1.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (3.5.0)\n",
            "Downloading scikit_learn-1.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "mlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed scikit-learn-1.2.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sklearn"
                ]
              },
              "id": "bbff458be070467eaf96717ebbea2d71"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial import cKDTree\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.ensemble import VotingRegressor\n",
        "\n",
        "# Load datasets\n",
        "train = pd.read_csv(\"Train.csv\")\n",
        "test = pd.read_csv(\"Test.csv\")\n",
        "toilets = pd.read_csv(\"toilets.csv\")\n",
        "waste_management = pd.read_csv(\"waste_management.csv\")\n",
        "water_sources = pd.read_csv(\"water_sources.csv\")\n",
        "\n",
        "# Combine train and test datasets for consistent preprocessing\n",
        "hospital_data = pd.concat([train, test])\n",
        "\n",
        "# Drop unnecessary columns from supplementary datasets\n",
        "for df in [toilets, waste_management, water_sources]:\n",
        "    df.drop(columns=['Year', 'Month'], inplace=True)\n",
        "\n",
        "# Rename columns for clarity\n",
        "def rename_columns(df, prefix):\n",
        "    for col in df.columns:\n",
        "        if col not in ['Month_Year_lat_lon', 'lat_lon']:\n",
        "            df.rename(columns={col: f\"{prefix}_{col}\"}, inplace=True)\n",
        "\n",
        "rename_columns(toilets, \"toilet\")\n",
        "rename_columns(waste_management, \"waste\")\n",
        "rename_columns(water_sources, \"water\")\n",
        "\n",
        "# Ensure unique identifier columns exist in all supplementary datasets\n",
        "\n",
        "\n",
        "\n",
        "# Fill missing values in the 'Total' column\n",
        "hospital_data['Total'].fillna(0, inplace=True)\n",
        "\n",
        "# Drop rows with missing latitude and longitude in water sources\n",
        "water_sources.dropna(subset=['water_Transformed_Latitude'], inplace=True)\n",
        "\n",
        "# Function to find nearest locations\n",
        "def find_nearest(hospital_data, location_df, lat_col, lon_col, id_col):\n",
        "    # Create a cKDTree for efficient nearest neighbour search\n",
        "    tree = cKDTree(location_df[[lat_col, lon_col]].values)\n",
        "    nearest = {}\n",
        "    # Loop through each hospital and find the nearest site in location_df\n",
        "    for _, row in hospital_data.iterrows():\n",
        "        _, idx = tree.query([row['Transformed_Latitude'], row['Transformed_Longitude']])\n",
        "        nearest[row['ID']] = location_df.iloc[idx][id_col]\n",
        "    return nearest\n",
        "\n",
        "for df, prefix in [(toilets, 'toilet'), (waste_management, 'waste'), (water_sources, 'water')]:\n",
        "   df[f\"{prefix}_Month_Year_lat_lon\"] = (\n",
        "      df[f\"{prefix}_Month_Year\"] + '_' +\n",
        "      df[f\"{prefix}_Transformed_Latitude\"].astype(str) + '_' +\n",
        "      df[f\"{prefix}_Transformed_Longitude\"].astype(str)\n",
        "    )\n",
        "\n",
        "\n",
        "# Merge datasets with nearest locations\n",
        "merged_data = hospital_data.copy()\n",
        "datasets = [\n",
        "    (toilets, 'toilet', 'toilet_Month_Year_lat_lon'),\n",
        "    (waste_management, 'waste', 'waste_Month_Year_lat_lon'),\n",
        "    (water_sources, 'water', 'water_Month_Year_lat_lon'),\n",
        "]\n",
        "\n",
        "for df, prefix, id_col in datasets:\n",
        "    nearest = find_nearest(merged_data, df, f\"{prefix}_Transformed_Latitude\", f\"{prefix}_Transformed_Longitude\", id_col)\n",
        "    nearest_df = pd.DataFrame(list(nearest.items()), columns=['ID', id_col])\n",
        "    merged_data = merged_data.merge(nearest_df, on=\"ID\").merge(df, on=id_col)\n",
        "\n",
        "# Split merged data into train and test sets\n",
        "train_df = merged_data[merged_data['Year'] < 2023]\n",
        "test_df = merged_data[merged_data['Year'] == 2023]\n",
        "\n",
        "# Specify the target column\n",
        "target_column = 'Total'\n",
        "\n",
        "# Feature and target split\n",
        "X = train_df.drop(columns=[target_column, 'ID', 'Location'], errors='ignore')\n",
        "y = train_df[target_column]\n",
        "\n",
        "# Handle categorical features\n",
        "categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col])\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Hyperparameter tuning for Random Forest\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "rf_params = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "}\n",
        "rf_grid = GridSearchCV(rf, rf_params, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "rf_grid.fit(X_train, y_train)\n",
        "best_rf = rf_grid.best_estimator_\n",
        "\n",
        "# Hyperparameter tuning for XGBoost\n",
        "xgb = XGBRegressor(random_state=42, verbosity=0)\n",
        "xgb_params = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [3, 6],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "    'subsample': [0.8, 1.0],\n",
        "}\n",
        "xgb_grid = GridSearchCV(xgb, xgb_params, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "xgb_grid.fit(X_train, y_train)\n",
        "best_xgb = xgb_grid.best_estimator_\n",
        "\n",
        "# Create a hybrid model using Voting Regressor\n",
        "hybrid_model = VotingRegressor([('RandomForest', best_rf), ('XGBoost', best_xgb)])\n",
        "hybrid_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = hybrid_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "\n",
        "# Prepare test data\n",
        "X_test_final = test_df.drop(columns=['Total', 'ID', 'Location'], errors='ignore')\n",
        "\n",
        "# Handle categorical features in test data\n",
        "for col in categorical_cols:\n",
        "    if col in X_test_final.columns:\n",
        "        le = LabelEncoder()\n",
        "        X_test_final[col] = le.fit_transform(X_test_final[col])\n",
        "\n",
        "# Align test dataset with training features\n",
        "for col in X.columns:\n",
        "    if col not in X_test_final.columns:\n",
        "        X_test_final[col] = 0  # Add missing feature with default value (e.g., zero)\n",
        "\n",
        "# Ensure columns are in the same order as training\n",
        "X_test_final = X_test_final[X.columns]\n",
        "\n",
        "# Make predictions on test data\n",
        "predictions = hybrid_model.predict(X_test_final)\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "test_predictions = pd.DataFrame({\n",
        "    'ID': test_df['ID'],\n",
        "    'Predictions': predictions\n",
        "})\n",
        "\n",
        "test_predictions.to_csv(\"test_predictions.csv\", index=False)\n",
        "print(\"Predictions saved to 'test_predictions.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "96C5dAzqjlb0",
        "outputId": "a5fa0fac-e5c1-4cc4-fb16-89dec7d42694"
      },
      "id": "96C5dAzqjlb0",
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-1-06f9426cbc46>:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  hospital_data['Total'].fillna(0, inplace=True)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-06f9426cbc46>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m }\n\u001b[1;32m    107\u001b[0m \u001b[0mrf_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'neg_mean_absolute_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m \u001b[0mrf_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0mbest_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1761\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MLflow"
      ],
      "metadata": {
        "id": "ra5PDEQ1iVkf"
      },
      "id": "ra5PDEQ1iVkf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlflow scikit-fuzzy deap xgboost"
      ],
      "metadata": {
        "id": "hRSx7POGi14F",
        "outputId": "d958e1c1-c460-4c41-8e8e-772ac21b74a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "hRSx7POGi14F",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mlflow\n",
            "  Downloading mlflow-2.20.0-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting scikit-fuzzy\n",
            "  Downloading scikit_fuzzy-0.5.0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting deap\n",
            "  Downloading deap-1.4.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.3)\n",
            "Collecting mlflow-skinny==2.20.0 (from mlflow)\n",
            "  Downloading mlflow_skinny-2.20.0-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.0)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.5)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
            "  Downloading alembic-1.14.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting graphene<4 (from mlflow)\n",
            "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting gunicorn<24 (from mlflow)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.7)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.26.4)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<19,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (17.0.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.6.1)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.13.1)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.37)\n",
            "Requirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.0->mlflow) (5.5.1)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.0->mlflow) (8.1.8)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.0->mlflow) (3.1.1)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.20.0->mlflow)\n",
            "  Downloading databricks_sdk-0.41.0-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.0->mlflow) (3.1.44)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.0->mlflow) (8.6.1)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.0->mlflow) (1.16.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.0->mlflow) (1.16.0)\n",
            "Requirement already satisfied: packaging<25 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.0->mlflow) (24.2)\n",
            "Requirement already satisfied: protobuf<6,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.0->mlflow) (4.25.5)\n",
            "Requirement already satisfied: pydantic<3,>=1.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.0->mlflow) (2.10.5)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.0->mlflow) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.0->mlflow) (2.32.3)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.0->mlflow) (0.5.3)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.20.0->mlflow) (4.12.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Collecting Mako (from alembic!=1.10.0,<2->mlflow)\n",
            "  Downloading Mako-1.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.3.0)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_core-3.2.5-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2<4,>=2.11->mlflow) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (4.55.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (3.2.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (3.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.1)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.0->mlflow) (2.27.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.20.0->mlflow) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.20.0->mlflow) (3.21.0)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.20.0->mlflow) (1.2.15)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.20.0->mlflow) (75.1.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.37b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.20.0->mlflow) (0.37b0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.0->mlflow-skinny==2.20.0->mlflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.0->mlflow-skinny==2.20.0->mlflow) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.20.0->mlflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.20.0->mlflow) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.20.0->mlflow) (2024.12.14)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.20.0->mlflow) (1.17.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.20.0->mlflow) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.0->mlflow) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.0->mlflow) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.0->mlflow) (0.6.1)\n",
            "Downloading mlflow-2.20.0-py3-none-any.whl (28.3 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m28.3/28.3 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-2.20.0-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_fuzzy-0.5.0-py2.py3-none-any.whl (920 kB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m920.8/920.8 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deap-1.4.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (135 kB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading databricks_sdk-0.41.0-py3-none-any.whl (637 kB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m637.5/637.5 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.5-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading Mako-1.3.8-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-fuzzy, Mako, gunicorn, graphql-core, deap, graphql-relay, docker, alembic, graphene, databricks-sdk, mlflow-skinny, mlflow\n",
            "Successfully installed Mako-1.3.8 alembic-1.14.1 databricks-sdk-0.41.0 deap-1.4.2 docker-7.1.0 graphene-3.4.3 graphql-core-3.2.5 graphql-relay-3.2.0 gunicorn-23.0.0 mlflow-2.20.0 mlflow-skinny-2.20.0 scikit-fuzzy-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datetime import datetime\n",
        "import logging\n",
        "import os\n",
        "\n",
        "# [Additional imports if needed]\n",
        "\n",
        "def identify_target_column(train_path):\n",
        "    \"\"\"\n",
        "    Identify the target column dynamically.\n",
        "\n",
        "    Args:\n",
        "        train_path (str): Path to the training dataset.\n",
        "\n",
        "    Returns:\n",
        "        str: Name of the target column.\n",
        "    \"\"\"\n",
        "    # Potential target column names\n",
        "    potential_targets = [\n",
        "        'Disease_Cases', 'Cases', 'Target', 'Outcome',\n",
        "        'Prediction', 'Label', 'Result'\n",
        "    ]\n",
        "\n",
        "    train = pd.read_csv(train_path)\n",
        "\n",
        "    # Log available columns for debugging\n",
        "    logging.info(f\"Available columns in train dataset: {list(train.columns)}\")\n",
        "\n",
        "    # Check for predefined target columns\n",
        "    for col in potential_targets:\n",
        "        if col in train.columns:\n",
        "            logging.info(f\"Found target column: {col}\")\n",
        "            return col\n",
        "\n",
        "    # Check for numeric columns as fallback\n",
        "    numeric_columns = train.select_dtypes(include=[np.number]).columns\n",
        "    numeric_target_candidates = [\n",
        "        col for col in numeric_columns\n",
        "        if col not in ['ID', 'Latitude', 'Longitude']\n",
        "    ]\n",
        "\n",
        "    if numeric_target_candidates:\n",
        "        target_column = numeric_target_candidates[0]\n",
        "        logging.warning(f\"No explicit target column found. Using: {target_column}\")\n",
        "        return target_column\n",
        "\n",
        "    raise ValueError(\"No suitable target column found in the dataset.\")\n",
        "\n",
        "class ImprovedDiseasePredictionPipeline:\n",
        "    def __init__(self, experiment_name, target_column):\n",
        "        self.experiment_name = experiment_name\n",
        "        self.target_column = target_column\n",
        "        # Initialize additional attributes if needed\n",
        "\n",
        "    def load_and_preprocess_data(self, train_path, test_path, toilets_path, waste_path, water_path):\n",
        "        \"\"\"\n",
        "        Load and preprocess datasets.\n",
        "        \"\"\"\n",
        "        train = pd.read_csv(train_path)\n",
        "        test = pd.read_csv(test_path)\n",
        "        # Load additional datasets and merge if required\n",
        "        # Preprocessing steps\n",
        "        return train, test\n",
        "\n",
        "    def train_hybrid_model(self, X_train, y_train, X_val, y_val):\n",
        "        \"\"\"\n",
        "        Train the hybrid model.\n",
        "        \"\"\"\n",
        "        # Example training process\n",
        "        mae, rmse, r2 = 0.0, 0.0, 0.0  # Replace with actual model training logic\n",
        "        return mae, rmse, r2\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        \"\"\"\n",
        "        Generate predictions for the test set.\n",
        "        \"\"\"\n",
        "        # Replace with model prediction logic\n",
        "        predictions = np.zeros(len(X_test))  # Placeholder\n",
        "        return predictions\n",
        "\n",
        "    def save_predictions(self, predictions, ids, output_path):\n",
        "        \"\"\"\n",
        "        Save predictions to a CSV file.\n",
        "        \"\"\"\n",
        "        output = pd.DataFrame({'ID': ids, 'Prediction': predictions})\n",
        "        output.to_csv(output_path, index=False)\n",
        "        logging.info(f\"Predictions saved to {output_path}\")\n",
        "\n",
        "    def save_model(self, model_path):\n",
        "        \"\"\"\n",
        "        Save the trained model.\n",
        "        \"\"\"\n",
        "        # Replace with actual model saving logic\n",
        "        os.makedirs(model_path, exist_ok=True)\n",
        "        logging.info(f\"Model saved to {model_path}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the disease prediction pipeline.\"\"\"\n",
        "    try:\n",
        "        # Set up logging\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "            handlers=[\n",
        "                logging.FileHandler('disease_prediction.log'),\n",
        "                logging.StreamHandler()\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Define data paths\n",
        "        data_paths = {\n",
        "            'train': \"Train.csv\",\n",
        "            'test': \"Test.csv\",\n",
        "            'toilets': \"toilets.csv\",\n",
        "            'waste': \"waste_management.csv\",\n",
        "            'water': \"water_sources.csv\"\n",
        "        }\n",
        "\n",
        "        # Dynamically identify the target column\n",
        "        target_column = identify_target_column(data_paths['train'])\n",
        "\n",
        "        # Initialize the pipeline\n",
        "        pipeline = ImprovedDiseasePredictionPipeline(\n",
        "            experiment_name=\"disease_prediction_experiment\",\n",
        "            target_column=target_column\n",
        "        )\n",
        "\n",
        "        # Load and preprocess data\n",
        "        logging.info(\"Loading and preprocessing data...\")\n",
        "        train, test = pipeline.load_and_preprocess_data(\n",
        "            data_paths['train'],\n",
        "            data_paths['test'],\n",
        "            data_paths['toilets'],\n",
        "            data_paths['waste'],\n",
        "            data_paths['water']\n",
        "        )\n",
        "\n",
        "        # Split data for training\n",
        "        logging.info(\"Splitting data for training...\")\n",
        "        try:\n",
        "            X_train = train.drop(['ID', target_column], axis=1)\n",
        "            y_train = train[target_column]\n",
        "            X_val, X_test, y_val, y_test = train_test_split(\n",
        "                X_train, y_train, test_size=0.2, random_state=42\n",
        "            )\n",
        "        except KeyError as e:\n",
        "            logging.error(f\"Column missing: {e}\")\n",
        "            raise\n",
        "\n",
        "        # Train the model\n",
        "        logging.info(\"Training hybrid model...\")\n",
        "        mae, rmse, r2 = pipeline.train_hybrid_model(X_train, y_train, X_val, y_val)\n",
        "\n",
        "        # Generate predictions for the test set\n",
        "        logging.info(\"Generating predictions for the test set...\")\n",
        "        X_test = test.drop(['ID'], axis=1)\n",
        "        predictions = pipeline.predict(X_test)\n",
        "\n",
        "        # Save predictions\n",
        "        output_path = f\"predictions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "        pipeline.save_predictions(predictions, test['ID'], output_path)\n",
        "\n",
        "        # Save the trained model\n",
        "        pipeline.save_model(\"models\")\n",
        "\n",
        "        logging.info(\"Pipeline execution completed successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Pipeline execution failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "TRA3lrJoih9y",
        "outputId": "70128db4-94b9-4b07-8d40-69e03cf34863",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "TRA3lrJoih9y",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:No explicit target column found. Using: Total\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# try"
      ],
      "metadata": {
        "id": "sJzeYWx4Xcs-"
      },
      "id": "sJzeYWx4Xcs-",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, VotingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "import logging\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Function to identify the target column\n",
        "def identify_target_column(train_path):\n",
        "    potential_targets = ['Disease_Cases', 'Cases', 'Target', 'Outcome', 'Prediction', 'Label', 'Result']\n",
        "    train = pd.read_csv(train_path)\n",
        "    logging.info(f\"Available columns in train dataset: {list(train.columns)}\")\n",
        "\n",
        "    for col in potential_targets:\n",
        "        if col in train.columns:\n",
        "            logging.info(f\"Found target column: {col}\")\n",
        "            return col\n",
        "\n",
        "    numeric_columns = train.select_dtypes(include=[np.number]).columns\n",
        "    numeric_target_candidates = [\n",
        "        col for col in numeric_columns if col not in ['ID', 'Latitude', 'Longitude']\n",
        "    ]\n",
        "\n",
        "    if numeric_target_candidates:\n",
        "        target_column = numeric_target_candidates[0]\n",
        "        logging.warning(f\"No explicit target column found. Using: {target_column}\")\n",
        "        return target_column\n",
        "\n",
        "    raise ValueError(\"No suitable target column found in the dataset.\")\n",
        "\n",
        "class ImprovedDiseasePredictionPipeline:\n",
        "    def __init__(self, experiment_name, target_column):\n",
        "        self.experiment_name = experiment_name\n",
        "        self.target_column = target_column\n",
        "\n",
        "    def load_and_preprocess_data(self, train_path, test_path, toilets_path, waste_path, water_path):\n",
        "        train = pd.read_csv(train_path)\n",
        "        test = pd.read_csv(test_path)\n",
        "        # Add any necessary preprocessing here\n",
        "        return train, test\n",
        "\n",
        "    def train_hybrid_model(self, X_train, y_train, X_val, y_val):\n",
        "        xgb = XGBRegressor(n_estimators=500, max_depth=6, learning_rate=0.05, random_state=42)\n",
        "        gbr = GradientBoostingRegressor(n_estimators=500, max_depth=4, learning_rate=0.05, random_state=42)\n",
        "        rf = RandomForestRegressor(n_estimators=500, max_depth=10, random_state=42)\n",
        "\n",
        "        model = VotingRegressor(estimators=[\n",
        "            ('xgb', xgb),\n",
        "            ('gbr', gbr),\n",
        "            ('rf', rf)\n",
        "        ])\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        y_pred = model.predict(X_val)\n",
        "        mae = mean_absolute_error(y_val, y_pred)\n",
        "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "        r2 = r2_score(y_val, y_pred)\n",
        "\n",
        "        logging.info(f\"Validation MAE: {mae}, RMSE: {rmse}, R2: {r2}\")\n",
        "        return mae, rmse, r2\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        predictions = self.model.predict(X_test)\n",
        "        return predictions\n",
        "\n",
        "    def save_predictions(self, predictions, ids, output_path):\n",
        "        output = pd.DataFrame({'ID': ids, 'Prediction': predictions})\n",
        "        output.to_csv(output_path, index=False)\n",
        "        logging.info(f\"Predictions saved to {output_path}\")\n",
        "\n",
        "    def save_model(self, model_path):\n",
        "        os.makedirs(model_path, exist_ok=True)\n",
        "        # Save models here\n",
        "        logging.info(f\"Model saved to {model_path}\")\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "            handlers=[\n",
        "                logging.FileHandler('disease_prediction.log'),\n",
        "                logging.StreamHandler()\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        data_paths = {\n",
        "            'train': \"Train.csv\",\n",
        "            'test': \"Test.csv\",\n",
        "            'toilets': \"toilets.csv\",\n",
        "            'waste': \"waste_management.csv\",\n",
        "            'water': \"water_sources.csv\"\n",
        "        }\n",
        "\n",
        "        target_column = identify_target_column(data_paths['train'])\n",
        "\n",
        "        pipeline = ImprovedDiseasePredictionPipeline(\n",
        "            experiment_name=\"disease_prediction_experiment\",\n",
        "            target_column=target_column\n",
        "        )\n",
        "\n",
        "        train, test = pipeline.load_and_preprocess_data(\n",
        "            data_paths['train'],\n",
        "            data_paths['test'],\n",
        "            data_paths['toilets'],\n",
        "            data_paths['waste'],\n",
        "            data_paths['water']\n",
        "        )\n",
        "\n",
        "        logging.info(\"Splitting data for training...\")\n",
        "        X_train = train.drop(['ID', target_column], axis=1)\n",
        "        y_train = train[target_column]\n",
        "        X_val, X_test, y_val, y_test = train_test_split(\n",
        "            X_train, y_train, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        logging.info(\"Training hybrid model...\")\n",
        "        mae, rmse, r2 = pipeline.train_hybrid_model(X_train, y_train, X_val, y_val)\n",
        "\n",
        "        logging.info(\"Generating predictions for the test set...\")\n",
        "        X_test = test.drop(['ID'], axis=1)\n",
        "        predictions = pipeline.predict(X_test)\n",
        "\n",
        "        output_path = f\"predictions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "        pipeline.save_predictions(predictions, test['ID'], output_path)\n",
        "\n",
        "        pipeline.save_model(\"models\")\n",
        "\n",
        "        logging.info(\"Pipeline execution completed successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Pipeline execution failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "_u0vYySPXgql",
        "outputId": "3ac47294-6475-42a2-dba4-675ade46f485"
      },
      "id": "_u0vYySPXgql",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:No explicit target column found. Using: Total\n",
            "ERROR:root:Pipeline execution failed: 'super' object has no attribute '__sklearn_tags__'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'super' object has no attribute '__sklearn_tags__'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-50642910e87c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-50642910e87c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training hybrid model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mmae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_hybrid_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generating predictions for the test set...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-50642910e87c>\u001b[0m in \u001b[0;36mtrain_hybrid_model\u001b[0;34m(self, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[1;32m     56\u001b[0m         ])\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_voting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sample_weight\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_voting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;34m\"\"\"Get common fit operations.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_estimators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_base.py\u001b[0m in \u001b[0;36m_validate_estimators\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mestimators\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mest\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"drop\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_estimator_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m                 raise ValueError(\n\u001b[1;32m    235\u001b[0m                     \"The estimator {} should be a {}.\".format(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mis_regressor\u001b[0;34m(estimator)\u001b[0m\n\u001b[1;32m   1275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_estimator_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"regressor\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1277\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"regressor\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_tags.py\u001b[0m in \u001b[0;36mget_tags\u001b[0;34m(estimator)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mklass\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmro\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"__sklearn_tags__\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mklass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m                 \u001b[0msklearn_tags_provider\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mklass\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__sklearn_tags__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m                 \u001b[0mclass_order\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mklass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m\"_more_tags\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mklass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m__sklearn_tags__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__sklearn_tags__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m         \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__sklearn_tags__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m         \u001b[0mtags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"regressor\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m         \u001b[0mtags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregressor_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRegressorTags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'super' object has no attribute '__sklearn_tags__'"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install scikit-learn==1.1.3\n",
        "!pip install xgboost==1.7.5"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "OO9GIeTaZ34-",
        "outputId": "c5aa80c6-73ba-44e1-f2a3-39d5a8d6b71d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        }
      },
      "id": "OO9GIeTaZ34-",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==1.1.3\n",
            "  Downloading scikit_learn-1.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.1.3) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.1.3) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.1.3) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.1.3) (3.5.0)\n",
            "Downloading scikit_learn-1.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32.0 MB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m32.0/32.0 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.1.3 which is incompatible.\n",
            "bigframes 1.33.0 requires scikit-learn>=1.2.2, but you have scikit-learn 1.1.3 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.1.3 which is incompatible.\n",
            "mlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.1.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed scikit-learn-1.1.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sklearn"
                ]
              },
              "id": "0beb2fe52ac1477c890bff18b224f1be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xgboost==1.7.5\n",
            "  Downloading xgboost-1.7.5-py3-none-manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost==1.7.5) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost==1.7.5) (1.13.1)\n",
            "Downloading xgboost-1.7.5-py3-none-manylinux2014_x86_64.whl (200.3 MB)\n",
            "\u001b[2K   \u001b[91mââââââââââââââââââââââââââââââââââââââ\u001b[0m\u001b[91mâ¸\u001b[0m\u001b[90mâ\u001b[0m \u001b[32m193.1/200.3 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[91mââââââââââââââââââââââââââââââââââââââ\u001b[0m\u001b[91mâ¸\u001b[0m\u001b[90mâ\u001b[0m \u001b[32m193.1/200.3 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, VotingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "import logging\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Function to identify the target column\n",
        "def identify_target_column(train_path):\n",
        "    potential_targets = ['Disease_Cases', 'Cases', 'Target', 'Outcome', 'Prediction', 'Label', 'Result']\n",
        "    train = pd.read_csv(train_path)\n",
        "    logging.info(f\"Available columns in train dataset: {list(train.columns)}\")\n",
        "\n",
        "    for col in potential_targets:\n",
        "        if col in train.columns:\n",
        "            logging.info(f\"Found target column: {col}\")\n",
        "            return col\n",
        "\n",
        "    numeric_columns = train.select_dtypes(include=[np.number]).columns\n",
        "    numeric_target_candidates = [\n",
        "        col for col in numeric_columns if col not in ['ID', 'Latitude', 'Longitude']\n",
        "    ]\n",
        "\n",
        "    if numeric_target_candidates:\n",
        "        target_column = numeric_target_candidates[0]\n",
        "        logging.warning(f\"No explicit target column found. Using: {target_column}\")\n",
        "        return target_column\n",
        "\n",
        "    raise ValueError(\"No suitable target column found in the dataset.\")\n",
        "\n",
        "class ImprovedDiseasePredictionPipeline:\n",
        "    def __init__(self, experiment_name, target_column):\n",
        "        self.experiment_name = experiment_name\n",
        "        self.target_column = target_column\n",
        "\n",
        "    def load_and_preprocess_data(self, train_path, test_path, toilets_path, waste_path, water_path):\n",
        "        train = pd.read_csv(train_path)\n",
        "        test = pd.read_csv(test_path)\n",
        "        # Add any necessary preprocessing here\n",
        "        return train, test\n",
        "\n",
        "    def train_hybrid_model(self, X_train, y_train, X_val, y_val):\n",
        "        xgb = XGBRegressor(n_estimators=500, max_depth=6, learning_rate=0.05, random_state=42)\n",
        "        gbr = GradientBoostingRegressor(n_estimators=500, max_depth=4, learning_rate=0.05, random_state=42)\n",
        "        rf = RandomForestRegressor(n_estimators=500, max_depth=10, random_state=42)\n",
        "\n",
        "        model = VotingRegressor(estimators=[\n",
        "            ('xgb', xgb),\n",
        "            ('gbr', gbr),\n",
        "            ('rf', rf)\n",
        "        ])\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        y_pred = model.predict(X_val)\n",
        "        mae = mean_absolute_error(y_val, y_pred)\n",
        "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "        r2 = r2_score(y_val, y_pred)\n",
        "\n",
        "        logging.info(f\"Validation MAE: {mae}, RMSE: {rmse}, R2: {r2}\")\n",
        "        return mae, rmse, r2\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        predictions = self.model.predict(X_test)\n",
        "        return predictions\n",
        "\n",
        "    def save_predictions(self, predictions, ids, output_path):\n",
        "        output = pd.DataFrame({'ID': ids, 'Prediction': predictions})\n",
        "        output.to_csv(output_path, index=False)\n",
        "        logging.info(f\"Predictions saved to {output_path}\")\n",
        "\n",
        "    def save_model(self, model_path):\n",
        "        os.makedirs(model_path, exist_ok=True)\n",
        "        # Save models here\n",
        "        logging.info(f\"Model saved to {model_path}\")\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "            handlers=[\n",
        "                logging.FileHandler('disease_prediction.log'),\n",
        "                logging.StreamHandler()\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        data_paths = {\n",
        "            'train': \"Train.csv\",\n",
        "            'test': \"Test.csv\",\n",
        "            'toilets': \"toilets.csv\",\n",
        "            'waste': \"waste_management.csv\",\n",
        "            'water': \"water_sources.csv\"\n",
        "        }\n",
        "\n",
        "        target_column = identify_target_column(data_paths['train'])\n",
        "\n",
        "        pipeline = ImprovedDiseasePredictionPipeline(\n",
        "            experiment_name=\"disease_prediction_experiment\",\n",
        "            target_column=target_column\n",
        "        )\n",
        "\n",
        "        train, test = pipeline.load_and_preprocess_data(\n",
        "            data_paths['train'],\n",
        "            data_paths['test'],\n",
        "            data_paths['toilets'],\n",
        "            data_paths['waste'],\n",
        "            data_paths['water']\n",
        "        )\n",
        "\n",
        "        logging.info(\"Splitting data for training...\")\n",
        "        X_train = train.drop(['ID', target_column], axis=1)\n",
        "        y_train = train[target_column]\n",
        "        X_val, X_test, y_val, y_test = train_test_split(\n",
        "            X_train, y_train, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        logging.info(\"Training hybrid model...\")\n",
        "        mae, rmse, r2 = pipeline.train_hybrid_model(X_train, y_train, X_val, y_val)\n",
        "\n",
        "        logging.info(\"Generating predictions for the test set...\")\n",
        "        X_test = test.drop(['ID'], axis=1)\n",
        "        predictions = pipeline.predict(X_test)\n",
        "\n",
        "        output_path = f\"predictions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "        pipeline.save_predictions(predictions, test['ID'], output_path)\n",
        "\n",
        "        pipeline.save_model(\"models\")\n",
        "\n",
        "        logging.info(\"Pipeline execution completed successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Pipeline execution failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "XbmwYa6DZ_OI",
        "outputId": "dc66e5cd-7c7a-44bf-ba11-6a3862dad50d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "id": "XbmwYa6DZ_OI",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:No explicit target column found. Using: Total\n",
            "ERROR:root:Pipeline execution failed: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:Location: object, Category_Health_Facility_UUID: object, Disease: object\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:Location: object, Category_Health_Facility_UUID: object, Disease: object",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-50642910e87c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-50642910e87c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training hybrid model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mmae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_hybrid_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generating predictions for the test set...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-50642910e87c>\u001b[0m in \u001b[0;36mtrain_hybrid_model\u001b[0;34m(self, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[1;32m     56\u001b[0m         ])\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_voting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    599\u001b[0m         \"\"\"\n\u001b[1;32m    600\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_voting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m     81\u001b[0m             )\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n\u001b[0m\u001b[1;32m     84\u001b[0m             delayed(_fit_single_estimator)(\n\u001b[1;32m     85\u001b[0m                 \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_base.py\u001b[0m in \u001b[0;36m_fit_single_estimator\u001b[0;34m(estimator, X, y, sample_weight, message_clsname, message)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1079\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbosity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbosity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingCallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvalsLog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m             train_dmatrix, evals = _wrap_evaluation_matrices(\n\u001b[0m\u001b[1;32m   1082\u001b[0m                 \u001b[0mmissing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36m_wrap_evaluation_matrices\u001b[0;34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001b[0m\n\u001b[1;32m    594\u001b[0m     \"\"\"Convert array_like evaluation matrices into DMatrix.  Perform validation on the\n\u001b[1;32m    595\u001b[0m     way.\"\"\"\n\u001b[0;32m--> 596\u001b[0;31m     train_dmatrix = create_dmatrix(\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36m_create_dmatrix\u001b[0;34m(self, ref, **kwargs)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_can_use_qdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_method\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbooster\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"gblinear\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                 return QuantileDMatrix(\n\u001b[0m\u001b[1;32m   1004\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnthread\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_bin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_bin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, max_bin, ref, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, data_split_mode)\u001b[0m\n\u001b[1;32m   1571\u001b[0m                 )\n\u001b[1;32m   1572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1573\u001b[0;31m         self._init(\n\u001b[0m\u001b[1;32m   1574\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m             \u001b[0mref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_init\u001b[0;34m(self, data, ref, enable_categorical, **meta)\u001b[0m\n\u001b[1;32m   1630\u001b[0m             \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m         )\n\u001b[0;32m-> 1632\u001b[0;31m         \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1633\u001b[0m         \u001b[0;31m# delay check_call to throw intermediate exception first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1634\u001b[0m         \u001b[0m_check_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0mexc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m  \u001b[0;31m# pylint: disable=raising-bad-type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, fn, dft_ret)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;31m# Defer the exception in order to return 0 and stop the iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_temporary_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/data.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m   1400\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mit\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m         \u001b[0minput_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minput_data\u001b[0;34m(data, feature_names, feature_types, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m                 \u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_codes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_temporary_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m                 new, cat_codes, feature_names, feature_types = _proxy_transform(\n\u001b[0m\u001b[1;32m    618\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                     \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/data.py\u001b[0m in \u001b[0;36m_proxy_transform\u001b[0;34m(data, feature_names, feature_types, enable_categorical)\u001b[0m\n\u001b[1;32m   1445\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_arrow_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1446\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_pandas_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1447\u001b[0;31m         df, feature_names, feature_types = _transform_pandas_df(\n\u001b[0m\u001b[1;32m   1448\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_categorical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1449\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/data.py\u001b[0m in \u001b[0;36m_transform_pandas_df\u001b[0;34m(data, enable_categorical, feature_names, feature_types, meta)\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0mmeta\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m ) -> Tuple[PandasTransformed, Optional[FeatureNames], Optional[FeatureTypes]]:\n\u001b[0;32m--> 603\u001b[0;31m     \u001b[0mpandas_check_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_categorical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmeta\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmeta\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_matrix_meta\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"DataFrame for {meta} cannot have multiple columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/data.py\u001b[0m in \u001b[0;36mpandas_check_dtypes\u001b[0;34m(data, enable_categorical)\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0;32mor\u001b[0m \u001b[0mis_pa_ext_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         ):\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0m_invalid_dataframe_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_pd_sparse_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/data.py\u001b[0m in \u001b[0;36m_invalid_dataframe_dtype\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"DataFrame.dtypes for data must be int, float, bool or category.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\"\"{type_err} {_ENABLE_CAT_ERR} {err}\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:Location: object, Category_Health_Facility_UUID: object, Disease: object"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}