{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:143: UserWarning: A NumPy version >=1.19.5 and <1.27.0 is required for this version of SciPy (detected version 2.2.1)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#import mlflow\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#import mlflow.sklearn\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cKDTree\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split, cross_val_score\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler, RobustScaler, LabelEncoder\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\scipy\\spatial\\__init__.py:105\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m=============================================================\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mSpatial algorithms and data structures (:mod:`scipy.spatial`)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03m   QhullError\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_kdtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ckdtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_qhull\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\scipy\\spatial\\_kdtree.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright Anne M. Archibald 2008\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Released under the scipy license\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ckdtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cKDTree, cKDTreeNode\n\u001b[0;32m      6\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminkowski_distance_p\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminkowski_distance\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance_matrix\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRectangle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKDTree\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mminkowski_distance_p\u001b[39m(x, y, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n",
      "File \u001b[1;32m_ckdtree.pyx:1\u001b[0m, in \u001b[0;36minit scipy.spatial._ckdtree\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from scipy.spatial import cKDTree\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.impute import KNNImputer\n",
    "import skfuzzy as fuzz\n",
    "from deap import base, creator, tools, algorithms\n",
    "import warnings\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class ANFISLayer:\n",
    "    def __init__(self, n_inputs, n_rules, learning_rate=0.01):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_rules = n_rules\n",
    "        self.learning_rate = learning_rate\n",
    "        self.membership_params = np.random.randn(n_inputs, n_rules, 3)  # center, width, type\n",
    "        self.consequence_params = np.random.randn(n_rules, n_inputs + 1)\n",
    "        \n",
    "    def membership_function(self, x, params):\n",
    "        center, width, mf_type = params\n",
    "        if mf_type > 0.5:  # Gaussian\n",
    "            return fuzz.gaussmf(x, center, width)\n",
    "        else:  # Bell-shaped\n",
    "            return fuzz.gbellmf(x, width, 2, center)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Layer 1: Fuzzification\n",
    "        membership_values = np.zeros((X.shape[0], self.n_inputs, self.n_rules))\n",
    "        for i in range(self.n_inputs):\n",
    "            for j in range(self.n_rules):\n",
    "                membership_values[:, i, j] = self.membership_function(X[:, i], \n",
    "                                                                    self.membership_params[i, j])\n",
    "        \n",
    "        # Layer 2: Rules\n",
    "        rule_outputs = np.prod(membership_values, axis=1)\n",
    "        \n",
    "        # Layer 3: Normalization\n",
    "        normalized_firing_strengths = rule_outputs / (np.sum(rule_outputs, axis=1, keepdims=True) + 1e-10)\n",
    "        \n",
    "        # Layer 4: Consequence\n",
    "        extended_X = np.column_stack([X, np.ones(X.shape[0])])\n",
    "        consequent_outputs = np.dot(extended_X, self.consequence_params.T)\n",
    "        \n",
    "        # Layer 5: Output\n",
    "        final_output = np.sum(normalized_firing_strengths * consequent_outputs, axis=1)\n",
    "        return final_output\n",
    "\n",
    "class GeneticOptimizer:\n",
    "    def __init__(self, population_size=50, n_generations=30):\n",
    "        self.population_size = population_size\n",
    "        self.n_generations = n_generations\n",
    "        creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
    "        creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "        \n",
    "    def optimize(self, anfis, X, y):\n",
    "        toolbox = base.Toolbox()\n",
    "        n_params = np.prod(anfis.membership_params.shape) + np.prod(anfis.consequence_params.shape)\n",
    "        \n",
    "        toolbox.register(\"attr_float\", np.random.uniform, -1, 1)\n",
    "        toolbox.register(\"individual\", tools.initRepeat, creator.Individual, \n",
    "                        toolbox.attr_float, n=n_params)\n",
    "        toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "        \n",
    "        def evaluate(individual):\n",
    "            # Update ANFIS parameters\n",
    "            membership_params = np.array(individual[:np.prod(anfis.membership_params.shape)])\n",
    "            consequence_params = np.array(individual[np.prod(anfis.membership_params.shape):])\n",
    "            anfis.membership_params = membership_params.reshape(anfis.membership_params.shape)\n",
    "            anfis.consequence_params = consequence_params.reshape(anfis.consequence_params.shape)\n",
    "            \n",
    "            # Calculate fitness\n",
    "            predictions = anfis.forward(X)\n",
    "            return (mean_absolute_error(y, predictions),)\n",
    "        \n",
    "        toolbox.register(\"evaluate\", evaluate)\n",
    "        toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "        toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=0.1, indpb=0.1)\n",
    "        toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "        \n",
    "        # Evolution\n",
    "        population = toolbox.population(n=self.population_size)\n",
    "        algorithms.eaSimple(population, toolbox, cxpb=0.7, mutpb=0.3, \n",
    "                          ngen=self.n_generations, verbose=False)\n",
    "\n",
    "class ImprovedDiseasePredictionPipeline:\n",
    "    def __init__(self, experiment_name=\"disease_prediction\"):\n",
    "        self.experiment_name = experiment_name\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        self.label_encoders = {}\n",
    "        \n",
    "    def load_and_preprocess_data(self, train_path, test_path, toilets_path, \n",
    "                                waste_path, water_path):\n",
    "        with mlflow.start_run(run_name=\"data_preprocessing\"):\n",
    "            # Load datasets\n",
    "            train = pd.read_csv(train_path)\n",
    "            test = pd.read_csv(test_path)\n",
    "            toilets = pd.read_csv(toilets_path)\n",
    "            waste = pd.read_csv(waste_path)\n",
    "            water = pd.read_csv(water_path)\n",
    "            \n",
    "            # Log data statistics\n",
    "            mlflow.log_param(\"train_shape\", train.shape)\n",
    "            mlflow.log_param(\"test_shape\", test.shape)\n",
    "            \n",
    "            # Clean and preprocess data\n",
    "            for df in [toilets, waste, water]:\n",
    "                df.drop(columns=['Year', 'Month'], inplace=True, errors='ignore')\n",
    "                \n",
    "            # Advanced feature engineering\n",
    "            self._create_temporal_features(train)\n",
    "            self._create_temporal_features(test)\n",
    "            self._create_spatial_features(train, [toilets, waste, water])\n",
    "            self._create_spatial_features(test, [toilets, waste, water])\n",
    "            \n",
    "            # Handle missing values using KNN imputation\n",
    "            imputer = KNNImputer(n_neighbors=5)\n",
    "            numeric_columns = train.select_dtypes(include=[np.number]).columns\n",
    "            train[numeric_columns] = imputer.fit_transform(train[numeric_columns])\n",
    "            test[numeric_columns] = imputer.transform(test[numeric_columns])\n",
    "            \n",
    "            # Scale features\n",
    "            self.scalers['standard'] = StandardScaler()\n",
    "            self.scalers['robust'] = RobustScaler()\n",
    "            \n",
    "            train[numeric_columns] = self.scalers['standard'].fit_transform(train[numeric_columns])\n",
    "            test[numeric_columns] = self.scalers['standard'].transform(test[numeric_columns])\n",
    "            \n",
    "            # Encode categorical variables\n",
    "            categorical_columns = train.select_dtypes(include=['object']).columns\n",
    "            for col in categorical_columns:\n",
    "                self.label_encoders[col] = LabelEncoder()\n",
    "                train[col] = self.label_encoders[col].fit_transform(train[col])\n",
    "                test[col] = self.label_encoders[col].transform(test[col])\n",
    "            \n",
    "            return train, test\n",
    "            \n",
    "    def _create_temporal_features(self, df):\n",
    "        if 'Date' in df.columns:\n",
    "            df['Date'] = pd.to_datetime(df['Date'])\n",
    "            df['Month'] = df['Date'].dt.month\n",
    "            df['Season'] = df['Date'].dt.month % 12 // 3 + 1\n",
    "            df['DayOfYear'] = df['Date'].dt.dayofyear\n",
    "            \n",
    "    def _create_spatial_features(self, df, auxiliary_dfs):\n",
    "        for aux_df in auxiliary_dfs:\n",
    "            if 'Latitude' in aux_df.columns and 'Longitude' in aux_df.columns:\n",
    "                tree = cKDTree(aux_df[['Latitude', 'Longitude']].values)\n",
    "                distances, _ = tree.query(df[['Latitude', 'Longitude']].values, k=3)\n",
    "                df[f'{aux_df}_nearest_dist'] = distances[:, 0]\n",
    "                df[f'{aux_df}_avg_3_nearest'] = distances.mean(axis=1)\n",
    "                \n",
    "    def train_hybrid_model(self, X_train, y_train, X_val, y_val):\n",
    "        with mlflow.start_run(run_name=\"model_training\"):\n",
    "            # Initialize base models\n",
    "            self.models['rf'] = RandomForestRegressor(n_estimators=200, max_depth=15)\n",
    "            self.models['xgb'] = XGBRegressor(n_estimators=200, learning_rate=0.05)\n",
    "            self.models['gbm'] = GradientBoostingRegressor(n_estimators=200)\n",
    "            \n",
    "            # Train base models\n",
    "            for name, model in self.models.items():\n",
    "                model.fit(X_train, y_train)\n",
    "                val_pred = model.predict(X_val)\n",
    "                mae = mean_absolute_error(y_val, val_pred)\n",
    "                mlflow.log_metric(f\"{name}_mae\", mae)\n",
    "            \n",
    "            # Initialize and train ANFIS-GA hybrid\n",
    "            anfis = ANFISLayer(n_inputs=X_train.shape[1], n_rules=5)\n",
    "            genetic_optimizer = GeneticOptimizer()\n",
    "            genetic_optimizer.optimize(anfis, X_train, y_train)\n",
    "            \n",
    "            # Create ensemble predictions\n",
    "            ensemble_predictions = np.zeros(len(X_val))\n",
    "            for model in self.models.values():\n",
    "                ensemble_predictions += model.predict(X_val)\n",
    "            anfis_predictions = anfis.forward(X_val)\n",
    "            \n",
    "            # Weighted combination\n",
    "            final_predictions = 0.7 * (ensemble_predictions / len(self.models)) + 0.3 * anfis_predictions\n",
    "            \n",
    "            # Log metrics\n",
    "            mae = mean_absolute_error(y_val, final_predictions)\n",
    "            rmse = np.sqrt(mean_squared_error(y_val, final_predictions))\n",
    "            r2 = r2_score(y_val, final_predictions)\n",
    "            \n",
    "            mlflow.log_metrics({\n",
    "                \"final_mae\": mae,\n",
    "                \"final_rmse\": rmse,\n",
    "                \"final_r2\": r2\n",
    "            })\n",
    "            \n",
    "            return mae, rmse, r2\n",
    "            \n",
    "    def predict(self, X_test):\n",
    "        ensemble_predictions = np.zeros(len(X_test))\n",
    "        for model in self.models.values():\n",
    "            ensemble_predictions += model.predict(X_test)\n",
    "        ensemble_predictions /= len(self.models)\n",
    "        \n",
    "        # Add ANFIS predictions\n",
    "        anfis_predictions = self.anfis.forward(X_test)\n",
    "        final_predictions = 0.7 * ensemble_predictions + 0.3 * anfis_predictions\n",
    "        \n",
    "        return final_predictions\n",
    "        \n",
    "    def save_predictions(self, predictions, ids, output_path):\n",
    "        submission = pd.DataFrame({\n",
    "            'ID': ids,\n",
    "            'Target': predictions\n",
    "        })\n",
    "        submission.to_csv(output_path, index=False)\n",
    "        mlflow.log_artifact(output_path)\n",
    "\n",
    "def main():\n",
    "    # Initialize pipeline\n",
    "    pipeline = ImprovedDiseasePredictionPipeline()\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    train, test = pipeline.load_and_preprocess_data(\n",
    "        \"Train.csv\", \"Test.csv\", \"toilets.csv\", \n",
    "        \"waste_management.csv\", \"water_sources.csv\"\n",
    "    )\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        train.drop(['ID', 'Total'], axis=1),\n",
    "        train['Total'],\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    mae, rmse, r2 = pipeline.train_hybrid_model(X_train, y_train, X_val, y_val)\n",
    "    print(f\"Validation MAE: {mae:.4f}\")\n",
    "    print(f\"Validation RMSE: {rmse:.4f}\")\n",
    "    print(f\"Validation R2: {r2:.4f}\")\n",
    "    \n",
    "    # Generate predictions\n",
    "    X_test = test.drop(['ID'], axis=1)\n",
    "    predictions = pipeline.predict(X_test)\n",
    "    \n",
    "    # Save predictions\n",
    "    pipeline.save_predictions(\n",
    "        predictions,\n",
    "        test['ID'],\n",
    "        f\"predictions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
